import asyncio
import json
from datetime import datetime

from typing import List, Optional

import httpx

import numpy as np

from motor.motor_asyncio import AsyncIOMotorDatabase

import re



from ..core.config import settings

from ..core.database import load_faiss_index

from ..models.document import DocumentInDB

from ..models.history import HistoryReference, create_history

from ..models.document import get_document_by_id, get_documents_by_user

from ..services.embedding import EmbeddingService


def detect_query_type_fast(question: str) -> str:
    """Enhanced query type detection v·ªõi support cho code v√† reasoning."""
    q = question.lower()
    
    # PRIORITY 1: Code analysis questions (T·∫ßng 3)
    code_patterns = [
        r'ph√¢n\s*t√≠ch.*?(code|ƒëo·∫°n\s*code|l·ªói)',
        r's·ª≠a.*?(code|l·ªói|bug)',
        r'ƒëo·∫°n\s*code.*?(sai|l·ªói|bug|ƒë√∫ng)',
        r'ch·∫•m\s*ƒëi·ªÉm.*?code',
        r'code.*?(c√≥\s*v·∫•n\s*ƒë·ªÅ|sai|l·ªói)',
        r't√¨m\s*l·ªói.*?code',
    ]
    if any(re.search(p, q) for p in code_patterns):
        return "CODE_ANALYSIS"
    
    # PRIORITY 2: Exercise generation (T·∫ßng 3)
    exercise_patterns = [
        r't·∫°o.*?b√†i\s*t·∫≠p',
        r'vi·∫øt.*?(function|h√†m).*?d·ª±a\s*tr√™n',
        r'√°p\s*d·ª•ng.*?(v√†o|ƒë·ªÉ.*?vi·∫øt).*?code',
        r'cho.*?v√≠\s*d·ª•.*?code',
        r'vi·∫øt.*?code.*?theo',
    ]
    if any(re.search(p, q) for p in exercise_patterns):
        return "EXERCISE_GENERATION"
    
    # PRIORITY 3: Multi-concept reasoning (T·∫ßng 4)
    reasoning_patterns = [
        r'd·ª±a\s*tr√™n.*?(v√†|,).*?(h√£y|vi·∫øt|gi·∫£i\s*th√≠ch)',
        r'k·∫øt\s*h·ª£p.*?(v√†|,)',
        r'√°p\s*d·ª•ng.*?(v√†|,)',
        r'(hoisting|scope|closure).*?(v√†|,).*(function|loop|variable)',
        r'gi·∫£i\s*th√≠ch.*?c∆°\s*ch·∫ø.*?(v√†|,)',
    ]
    if any(re.search(p, q) for p in reasoning_patterns):
        return "MULTI_CONCEPT_REASONING"
    
    # PRIORITY 4: Section query - CRITICAL FIX: More comprehensive patterns
    section_patterns = [
        r'(ph·∫ßn|ch∆∞∆°ng|part)\s+\d+\s+(c√≥|n√≥i|l√†|g·ªìm)',
        r'n·ªôi\s*dung\s+(ph·∫ßn|ch∆∞∆°ng|part)\s+\d+',
        r'chi\s*ti·∫øt.*?(ph·∫ßn|ch∆∞∆°ng|part)\s+\d+',
        r'r√µ\s+h∆°n.*?(ph·∫ßn|ch∆∞∆°ng|part)\s+\d+',
        # NEW PATTERNS - More comprehensive
        r'n·ªôi\s*dung.*?(ph·∫ßn|ch∆∞∆°ng|part)\s+\d+',
        r'(ph·∫ßn|ch∆∞∆°ng|part)\s+\d+.*?(g√¨|n√†o|nh·ªØng\s*g√¨)',
        r'trong\s+(ph·∫ßn|ch∆∞∆°ng|part)\s+\d+',
        r'(ph·∫ßn|ch∆∞∆°ng|part)\s+\d+\s+bao\s*g·ªìm',
        r't√¨m\s*hi·ªÉu.*?(ph·∫ßn|ch∆∞∆°ng|part)\s+\d+',
        r'gi·ªõi\s*thi·ªáu.*?(ph·∫ßn|ch∆∞∆°ng|part)\s+\d+',
    ]
    if any(re.search(p, q) for p in section_patterns):
        return "SECTION_OVERVIEW"
    
    # PRIORITY 5: Document overview
    overview_patterns = [
        r'trong\s+(file|t√†i\s*li·ªáu)\s+n√†y\s+c√≥\s+g√¨',
        r'(file|t√†i\s*li·ªáu)\s+n√†y\s+(n√≥i|vi·∫øt|ƒë·ªÅ\s*c·∫≠p)\s+v·ªÅ\s+g√¨',
        r't·ªïng\s*quan\s+(file|t√†i\s*li·ªáu)',
        r'm·ª•c\s*l·ª•c',
    ]
    if any(re.search(p, q) for p in overview_patterns):
        return "DOCUMENT_OVERVIEW"
    
    # PRIORITY 6: Comparative/synthesis questions (T·∫ßng 2)
    comparative_patterns = [
        r'so\s*s√°nh',
        r'kh√°c.*?g√¨',
        r'gi·ªëng.*?g√¨',
        r'ph√¢n\s*bi·ªát',
        r'(s·ª±\s*)?kh√°c\s*nhau.*?gi·ªØa',
        r'g·ªôp.*?ki·∫øn\s*th·ª©c',
        r'k·∫øt\s*h·ª£p.*?t·ª´',
    ]
    if any(re.search(p, q) for p in comparative_patterns):
        return "COMPARE_SYNTHESIZE"
    
    # PRIORITY 7: List/enumerate questions
    if any(kw in q for kw in ["li·ªát k√™", "cho v√≠ d·ª•", "v√≠ d·ª• cho", "bao nhi√™u"]):
        if "h√£y li·ªát k√™" in q or "cho v√≠ d·ª•" in q or "li·ªát k√™" in q:
            return "EXPAND"
    
    # PRIORITY 8: Existence check
    if any(kw in q for kw in ["c√≥ ƒë·ªÅ c·∫≠p", "c√≥ n√≥i", "t√†i li·ªáu c√≥"]):
        return "EXISTENCE"
    
    # PRIORITY 9: Explanation/elaboration
    if any(kw in q for kw in ["gi·∫£i th√≠ch", "r√µ h∆°n", "t·∫°i sao", "v√≠ d·ª•"]):
        return "EXPAND"
    
    return "DIRECT"


def build_gemini_optimized_prompt(
    question: str,
    context_text: str,
    chunk_similarities: List[float],
    query_type: str = "DIRECT"
) -> str:
    """
    Gemini 2.5 Flash optimized prompt - SHORT, STRICT, STRUCTURED.
    Target: <1000 tokens for system instructions.
    """
    
    # Auto-detect query type
    q_lower = question.lower()
    
    # CRITICAL FIX: M·ªü r·ªông patterns cho SECTION_OVERVIEW detection
    section_query_patterns = [
        r'(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+)\s+(c√≥|n√≥i|l√†|g·ªìm)',  # "ph·∫ßn 8 c√≥ g√¨"
        r'n·ªôi\s*dung\s+(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+)',      # "n·ªôi dung ph·∫ßn 8"
        r'(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+)\s+.*?(g√¨|n√†o)',      # "ph·∫ßn 8 n√≥i g√¨"
        # TH√äM C√ÅC PATTERN M·ªöI
        r'chi\s*ti·∫øt.*?(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+)',       # "chi ti·∫øt v·ªÅ ph·∫ßn 8"
        r'(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+).*?chi\s*ti·∫øt',       # "ph·∫ßn 8 chi ti·∫øt"
        r'gi·∫£i\s*th√≠ch.*?(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+)',     # "gi·∫£i th√≠ch ph·∫ßn 8"
        r'(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+).*?gi·∫£i\s*th√≠ch',     # "ph·∫ßn 8 gi·∫£i th√≠ch"
        r't√¨m\s*hi·ªÉu.*?(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+)',       # "t√¨m hi·ªÉu ph·∫ßn 8"
        r'(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+).*?t√¨m\s*hi·ªÉu',       # "ph·∫ßn 8 t√¨m hi·ªÉu"
        r'v·ªÅ\s+(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+)',               # "v·ªÅ ph·∫ßn 8", "chi ti·∫øt h∆°n v·ªÅ ph·∫ßn 8"
        r'(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+)\s+l√†\s+g√¨',          # "ph·∫ßn 8 l√† g√¨"
        r'(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+)\s+n√≥i\s+v·ªÅ',         # "ph·∫ßn 8 n√≥i v·ªÅ"
        r'(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+)\s+bao\s+g·ªìm',        # "ph·∫ßn 8 bao g·ªìm"
        # CRITICAL: Th√™m patterns cho c√¢u c√≥ "chi ti·∫øt h∆°n"
        r'chi\s*ti·∫øt\s+h∆°n.*?(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+)',  # "chi ti·∫øt h∆°n PH·∫¶N 8"
        r'r√µ\s+h∆°n.*?(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+)',          # "r√µ h∆°n v·ªÅ PH·∫¶N 8"
        r'n√≥i\s+r√µ.*?(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+)',          # "n√≥i r√µ PH·∫¶N 8"
        # Pattern ƒë·∫∑c bi·ªát: b·∫Øt c·∫£ format "PH·∫¶N 8: Title"
        r'(ph·∫ßn|ch∆∞∆°ng|part)\s+(\d+)[:Ôºö]',                 # "PH·∫¶N 8:" or "PH·∫¶N 8Ôºö"
    ]
    
    is_section_query = False
    section_match = None
    section_num = None
    for pattern in section_query_patterns:
        match = re.search(pattern, q_lower)
        if match:
            is_section_query = True
            section_match = match
            # L·∫•y section number t·ª´ group ph√π h·ª£p (c√≥ th·ªÉ l√† group 1 ho·∫∑c 2 t√πy pattern)
            groups = match.groups()
            for i, group in enumerate(groups, 1):
                if group and group.isdigit():
                    section_num = group
                    break
            # Fallback: n·∫øu kh√¥ng t√¨m th·∫•y s·ªë trong groups, th·ª≠ extract t·ª´ match string
            if not section_num:
                # Extract s·ªë t·ª´ to√†n b·ªô match
                num_match = re.search(r'\d+', match.group(0))
                if num_match:
                    section_num = num_match.group(0)
            break
    
    # Detect other query modes
    is_overview = any(re.search(p, q_lower) for p in [
        r'trong\s+(file|t√†i\s*li·ªáu)\s+n√†y\s+c√≥\s+g√¨',
        r'(file|t√†i\s*li·ªáu)\s+n√†y\s+(n√≥i|vi·∫øt|ƒë·ªÅ\s*c·∫≠p)\s+v·ªÅ\s+g√¨',
        r't·ªïng\s*quan\s+(file|t√†i\s*li·ªáu)',
        r'm·ª•c\s*l·ª•c',
    ])
    
    # Determine mode based on query_type
    if query_type == "CODE_ANALYSIS":
        mode = "CODE_ANALYSIS"
        existence_subtype = None
    elif query_type == "EXERCISE_GENERATION":
        mode = "EXERCISE_GENERATION"
        existence_subtype = None
    elif query_type == "MULTI_CONCEPT_REASONING":
        mode = "MULTI_CONCEPT_REASONING"
        existence_subtype = None
    elif query_type == "COMPARE_SYNTHESIZE":
        mode = "COMPARE_SYNTHESIZE"
        existence_subtype = None
    elif is_section_query:
        mode = "SECTION_OVERVIEW"
        existence_subtype = None
    elif is_overview:
        mode = "DOCUMENT_OVERVIEW"
        existence_subtype = None
    else:
        # Fallback to old detection logic
        is_too_broad = any(kw in q_lower for kw in [
            "to√†n b·ªô", "t·∫•t c·∫£ m·ªçi", "every", "all"
        ])
        is_list_all = any(kw in q_lower for kw in ["li·ªát k√™", "cho v√≠ d·ª•", "bao nhi√™u"])
        is_expanded = is_list_all or any(kw in q_lower for kw in ["gi·∫£i th√≠ch", "r√µ h∆°n", "t·∫°i sao"])
        is_existence = any(kw in q_lower for kw in ["c√≥ ƒë·ªÅ c·∫≠p", "c√≥ n√≥i", "t√†i li·ªáu c√≥"])
        is_comparative = any(kw in q_lower for kw in ["so s√°nh", "kh√°c", "gi·ªëng"])
        
        if is_too_broad:
            mode = "TOO_BROAD"
            existence_subtype = None
        elif is_expanded:
            mode = "EXPAND"
            existence_subtype = None
        elif is_existence:
            mode = "EXISTENCE"
            is_mention_only = any(kw in q_lower for kw in ["c√≥ ƒë·ªÅ c·∫≠p", "c√≥ nh·∫Øc ƒë·∫øn", "c√≥ n√≥i ƒë·∫øn"])
            existence_subtype = "MENTION_ONLY" if is_mention_only else "EXPLAINS"
        elif is_comparative:
            mode = "COMPARE"
            existence_subtype = None
        else:
            mode = "DIRECT"
            existence_subtype = None
    
    # Check similarity threshold
    max_sim = max(chunk_similarities) if chunk_similarities else 0
    auto_fallback_warning = ""
    if max_sim < 0.4:
        auto_fallback_warning = "\n‚ö†Ô∏è WARNING: Max similarity < 0.4 ‚Üí Must return FALLBACK."
    
    # Build mode-specific instructions
    mode_instructions = ""
    
    if mode == "DOCUMENT_OVERVIEW":
        mode_instructions = """

## üìö DOCUMENT OVERVIEW MODE

User is asking for a complete overview of the entire document.

**MANDATORY STEPS:**
1. **Find TABLE OF CONTENTS chunk:** Look for chunks containing "M·ª§C L·ª§C" or multiple "PH·∫¶N X"
2. **List ALL main sections:** Extract all section headings (PH·∫¶N 1, PH·∫¶N 2, ..., PH·∫¶N 10)
3. **Describe each section:** Provide 1-2 sentences describing what each section covers
4. **Use subsection info:** If available, mention key subsections (e.g., "8.1 String, 8.2 Function")

**OUTPUT FORMAT:**
```
T√†i li·ªáu n√†y bao g·ªìm c√°c ph·∫ßn sau:

1. **PH·∫¶N 1: [Title from document]** - [2-3 sentences describing content]

2. **PH·∫¶N 2: [Title]** - [2-3 sentences describing content]

3. **PH·∫¶N 3: [Title]** - [2-3 sentences describing content]

...

10. **PH·∫¶N 10: [Title]** - [2-3 sentences describing content]

[Cite chunks used]
```

**CRITICAL RULES:**
- MUST list ALL main sections (don't skip any)
- Use section titles from document (don't invent titles)
- If TABLE OF CONTENTS chunk exists, prioritize it
- Confidence should be 0.95 if TABLE OF CONTENTS found, 0.85 otherwise

**CRITICAL OUTPUT FORMAT (MUST BE VALID JSON):**
```json
{{
  "answer": "T√†i li·ªáu n√†y bao g·ªìm c√°c ph·∫ßn sau:\\n\\n1. **PH·∫¶N 1: [Title]** - [description]\\n\\n2. **PH·∫¶N 2: [Title]** - [description]\\n\\n...",
  "answer_type": "DOCUMENT_OVERVIEW",
  "chunks_used": [chunk_numbers],
  "confidence": 0.90-0.95,
  "sentence_mapping": [...],
  "sources": {{"from_document": true, "from_external_knowledge": false}}
}}
```

‚ö†Ô∏è CRITICAL: Output MUST be valid JSON. NO markdown blocks, NO extra text.

"""
    elif mode == "CODE_ANALYSIS":
        mode_instructions = """

## üîç CODE ANALYSIS MODE (T·∫ßng 3)

User is asking to analyze code based on document knowledge.

**MANDATORY STEPS:**
1. **Extract Concepts**: Identify relevant concepts from chunks (e.g., "scope", "hoisting", "closure")
2. **Apply to Code**: Apply these concepts to analyze the provided code
3. **Step-by-step Reasoning**: 
   - What does each line do?
   - What concept from the document applies here?
   - What's the issue/what works correctly?
4. **Explanation**: Explain clearly with references to document

**OUTPUT FORMAT:**
```
Ph√¢n t√≠ch code:

[Code snippet v·ªõi line numbers]

1. D√≤ng X: [Gi·∫£i th√≠ch d·ª±a tr√™n concept t·ª´ document]
2. D√≤ng Y: [V·∫•n ƒë·ªÅ/ƒêi·ªÉm ƒë√∫ng + t·∫°i sao]

K·∫øt lu·∫≠n: [T√≥m t·∫Øt + ƒë·ªÅ xu·∫•t s·ª≠a n·∫øu c√≥ l·ªói]

[Cite chunks used]
```

**CRITICAL RULES:**
- DO NOT just quote chunks - apply concepts to analyze
- Show reasoning process clearly
- If code has errors, explain WHY based on document knowledge
- Confidence should be 0.8-0.95 if concepts found in document

**CRITICAL OUTPUT FORMAT (MUST BE VALID JSON):**
```json
{{
  "answer": "Ph√¢n t√≠ch code:\\n\\n1. D√≤ng X: [explanation]\\n2. Problem: [explanation]",
  "answer_type": "CODE_ANALYSIS",
  "chunks_used": [chunk_numbers],
  "confidence": 0.75-0.85,
  "reasoning_steps": [
    "Step 1: Identify concepts used",
    "Step 2: Apply to code",
    "Step 3: Explain issue"
  ],
  "sentence_mapping": [...],
  "sources": {{"from_document": true, "from_external_knowledge": false}}
}}
```

‚ö†Ô∏è CRITICAL: Output MUST be valid JSON. NO markdown blocks, NO extra text.

"""
    elif mode == "EXERCISE_GENERATION":
        mode_instructions = """

## üìù EXERCISE GENERATION MODE (T·∫ßng 3)

User wants to create code/exercises based on document concepts.

**MANDATORY STEPS:**
1. **Understand Concepts**: Extract relevant concepts from chunks
2. **Synthesize**: Combine concepts to create new examples
3. **Create Code**: Write NEW code (not from document) applying these concepts
4. **Explain**: Link each part of code to concepts in document

**OUTPUT FORMAT:**
```
D·ª±a tr√™n ki·∫øn th·ª©c v·ªÅ [concepts] trong t√†i li·ªáu:

[New code here]

Gi·∫£i th√≠ch t·ª´ng ph·∫ßn:
- Line X-Y: √Åp d·ª•ng [concept from chunk Z]
- Line A-B: K·∫øt h·ª£p [concept 1] v√† [concept 2]

[Cite chunks used]
```

**CRITICAL RULES:**
- Code must be NEW, not copied from document
- MUST explain how each part relates to document concepts
- If combining multiple concepts, cite multiple chunks
- Confidence: 0.8-0.9 if concepts clearly found

"""
    elif mode == "MULTI_CONCEPT_REASONING":
        mode_instructions = """

## üß† MULTI-CONCEPT REASONING MODE (T·∫ßng 4)

User asks question requiring reasoning across multiple concepts.

**MANDATORY STEPS:**
1. **Identify Concepts**: List all concepts mentioned in question
2. **Extract from Document**: Find chunks for EACH concept
3. **Connect**: Show how concepts relate to each other
4. **Synthesize**: Combine understanding to answer question
5. **Reason**: Apply logic beyond just quoting

**OUTPUT FORMAT:**
```
ƒê·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y, c·∫ßn k·∫øt h·ª£p c√°c kh√°i ni·ªám:

1. [Concept 1] (t·ª´ chunk X): [Brief explanation]
2. [Concept 2] (t·ª´ chunk Y): [Brief explanation]

K·∫øt n·ªëi c√°c kh√°i ni·ªám:
[Explain how they relate, with reasoning]

√Åp d·ª•ng v√†o c√¢u h·ªèi:
[Answer with synthesis of concepts]

[Cite all chunks used]
```

**CRITICAL RULES:**
- MUST cite chunks for EACH concept
- Show reasoning process, not just quotes
- If concepts from different sections, cite both
- Confidence: 0.7-0.85 (reasoning adds uncertainty)
- If any concept missing from document ‚Üí note it explicitly

**CRITICAL OUTPUT FORMAT (MUST BE VALID JSON):**
```json
{{
  "answer": "ƒê·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y, c·∫ßn k·∫øt h·ª£p c√°c kh√°i ni·ªám:\\n\\n1. [Concept 1] (t·ª´ chunk X): [Brief explanation]\\n2. [Concept 2] (t·ª´ chunk Y): [Brief explanation]\\n\\nK·∫øt n·ªëi c√°c kh√°i ni·ªám:\\n[Explain how they relate, with reasoning]\\n\\n√Åp d·ª•ng v√†o c√¢u h·ªèi:\\n[Answer with synthesis of concepts]",
  "answer_type": "MULTI_CONCEPT_REASONING",
  "chunks_used": [chunk_numbers],
  "confidence": 0.7-0.85,
  "reasoning_steps": [
    "Step 1: Identify all concepts",
    "Step 2: Extract from document",
    "Step 3: Connect concepts",
    "Step 4: Synthesize answer"
  ],
  "sentence_mapping": [...],
  "sources": {{"from_document": true, "from_external_knowledge": false}}
}}
```

‚ö†Ô∏è CRITICAL: Output MUST be valid JSON. NO markdown blocks, NO extra text.

"""
    elif mode == "COMPARE_SYNTHESIZE":
        mode_instructions = """

## üîÄ COMPARE & SYNTHESIZE MODE (T·∫ßng 2)

User wants to compare concepts or synthesize knowledge from multiple sections.

**MANDATORY STEPS:**
1. **Find All Relevant Chunks**: Search for ALL mentioned concepts/items
2. **Extract Key Points**: For each concept, list main points
3. **Compare**: Show similarities and differences
4. **Synthesize**: Create coherent understanding

**OUTPUT FORMAT (for comparison):**
```
So s√°nh [A] v√† [B]:

**Gi·ªëng nhau:**
- [Point from chunks X, Y]

**Kh√°c nhau:**
| Aspect | [A] | [B] |
|--------|-----|-----|
| [Aspect 1] | [from chunk X] | [from chunk Y] |

[Cite chunks used]
```

**CRITICAL RULES:**
- MUST find chunks for ALL items being compared
- If one item has more chunks, it's OK - use what's available
- Show explicit citations for each point
- Confidence: 0.7-0.9 depending on chunk coverage

"""
    elif is_section_query and section_num:
        mode_instructions = f"""

## üéØ SECTION OVERVIEW QUERY DETECTED (MANDATORY FORMAT)

User is asking: "PH·∫¶N {section_num} n√≥i v·ªÅ g√¨?"

**CRITICAL RULES - VIOLATION = SYSTEM FAILURE:**
1. ‚úÖ YOU MUST return answer_type = "SECTION_OVERVIEW"
2. ‚úÖ YOU MUST NOT return "FALLBACK" or "TOO_BROAD"
3. ‚úÖ Chunks about PH·∫¶N {section_num} ARE AVAILABLE - use them!

**MANDATORY OUTPUT STRUCTURE:**
```
PH·∫¶N {section_num}: [Extract FULL section title from chunks]

N·ªôi dung ch√≠nh bao g·ªìm:

1. **[Topic 1 name]** - [2-3 sentences explaining this topic using info from chunks]

2. **[Topic 2 name]** - [2-3 sentences explaining this topic]

3. **[Topic 3 name]** - [2-3 sentences explaining this topic]

...

[Minimum 4-6 topics, cite chunk numbers used]
```

**EXTRACTION RULES:**
- Search chunks for heading markers: "PH·∫¶N {section_num}:", section titles, etc.
- Extract section title from heading chunk
- Extract **ALL subsection headings** (e.g., 8.1, 8.2, 8.3...) as topics
- For each topic, synthesize 2-3 sentences from related chunks
- **CRITICAL:** Don't stop at 2 topics - find ALL subsections

**CONFIDENCE RULES:**
- If you found chunks with "PH·∫¶N {section_num}" ‚Üí confidence = 0.90-0.95
- If chunks contain subsection numbers (8.1, 8.2) ‚Üí confidence = 0.95
- NEVER return confidence < 0.85 for section queries

**DEBUG CHECK:**
- Did I extract the section title? ‚úÖ/‚ùå
- Did I list ALL subsections (not just 2)? ‚úÖ/‚ùå
- Did I write 2-3 sentences per topic? ‚úÖ/‚ùå
- Did I cite chunk numbers? ‚úÖ/‚ùå

"""
    
    prompt = f"""# SYSTEM RULES (DO NOT describe these rules, just follow them)

{mode_instructions}

## HARD FAILS (Violate any ‚Üí immediate FALLBACK)
1. DO NOT answer if info not in chunks
2. DO NOT synthesize meaning from multiple unrelated chunks UNLESS in REASONING mode
3. DO NOT infer from headings/numbering EXCEPT for SECTION_OVERVIEW
4. If similarity < 0.4 for ALL chunks ‚Üí FALLBACK required{auto_fallback_warning}

## MODE: {mode}
- CODE_ANALYSIS: Extract concepts ‚Üí Apply to code ‚Üí Step-by-step reasoning ‚Üí Cite chunks
- EXERCISE_GENERATION: Understand concepts ‚Üí Create NEW code ‚Üí Explain links to document
- MULTI_CONCEPT_REASONING: Identify concepts ‚Üí Extract from doc ‚Üí Connect ‚Üí Synthesize ‚Üí Reason
- COMPARE_SYNTHESIZE: Find all chunks ‚Üí Extract points ‚Üí Compare/synthesize ‚Üí Cite all
- SECTION_OVERVIEW: Full title + detailed numbered list (4-6 items, 2-3 sentences each)
- DOCUMENT_OVERVIEW: List main sections with descriptions
- DIRECT: Use only document text (4-6 sentences max)
- EXPAND: List ALL items if "li·ªát k√™", or explain with examples
- COMPARE: Bullet list format
- EXISTENCE: Check if mentioned vs. explained in detail
- TOO_BROAD: Return "C√¢u h·ªèi qu√° r·ªông. Vui l√≤ng h·ªèi v·ªÅ ch·ªß ƒë·ªÅ c·ª• th·ªÉ."

## REASONING GUIDELINES (for Tier 3-4 questions)
For CODE_ANALYSIS, EXERCISE_GENERATION, MULTI_CONCEPT_REASONING modes:
1. **You MAY use logical reasoning** beyond just quoting chunks
2. **You MAY create new examples** based on concepts from document
3. **You MUST cite** which chunks provided the base knowledge
4. **Mark synthesis**: Use phrases like "√Åp d·ª•ng [concept t·ª´ chunk X]" or "D·ª±a tr√™n [concept], ta suy ra..."

Example (CODE_ANALYSIS):
```
‚ùå BAD: "Chunk 5 n√≥i v·ªÅ scope. ƒêo·∫°n code n√†y c√≥ l·ªói scope."
‚úÖ GOOD: "D·ª±a tr√™n kh√°i ni·ªám scope trong chunk 5 (bi·∫øn var c√≥ function scope), ta th·∫•y d√≤ng `console.log(i)` s·∫Ω b√°o l·ªói v√¨ `i` ƒë∆∞·ª£c khai b√°o v·ªõi `let` trong v√≤ng for (block scope), kh√¥ng truy c·∫≠p ƒë∆∞·ª£c b√™n ngo√†i."
```

## SELF-CHECK (MANDATORY before output)
1. Does answer directly address the question type?
2. For CODE/EXERCISE/REASONING: Did I show reasoning process?
3. For COMPARE: Did I find chunks for ALL items?
4. Are all citations correct and specific?
5. If FALLBACK ‚Üí chunks_used MUST be []

## CHUNKS

{context_text}

## QUESTION

{question}

## OUTPUT (JSON ONLY - no markdown, no comments, no extra text)

‚ö†Ô∏è CRITICAL: You MUST return ONLY a valid JSON object. NO text before or after JSON.
‚ö†Ô∏è DO NOT include markdown code blocks (```json).
‚ö†Ô∏è DO NOT include any explanation outside the JSON object.

Example of CORRECT output:
{{
  "answer": "PH·∫¶N 8: C√ö PH√ÅP ES6\\n\\nN·ªôi dung ch√≠nh bao g·ªìm:\\n\\n1. **String** - Template Literals...",
  "answer_type": "SECTION_OVERVIEW",
  "chunks_used": [204, 205, 208],
  "confidence": 0.95,
  "sentence_mapping": [{{"sentence": "first sentence", "chunk": 204, "external": false}}],
  "sources": {{"from_document": true, "from_external_knowledge": false}}
}}

Example of WRONG output (NEVER do this):
"Here is the answer: PH·∫¶N 8..." ‚Üê NO! This is not JSON!

{{
  "answer": "string",
  "answer_type": "CODE_ANALYSIS|EXERCISE_GENERATION|MULTI_CONCEPT_REASONING|COMPARE_SYNTHESIZE|SECTION_OVERVIEW|DOCUMENT_OVERVIEW|DIRECT|EXPAND|COMPARE|EXISTENCE|TOO_BROAD|FALLBACK",
  "chunks_used": [integers],
  "confidence": 0.0-1.0,
  "sentence_mapping": [
    {{"sentence": "first sentence of answer", "chunk": 1, "external": false}},
    {{"sentence": "second sentence", "chunk": 3, "external": false}}
  ],
  "sources": {{"from_document": bool, "from_external_knowledge": bool}}
}}

CRITICAL: 
- If FALLBACK ‚Üí chunks_used=[], confidence=0.0
- If CODE_ANALYSIS/REASONING ‚Üí include reasoning_steps
- If COMPARE_SYNTHESIZE with missing concept ‚Üí note in answer, confidence < 0.7
"""
    return prompt





class RAGService:

    def __init__(self):

        self.embedding_service = EmbeddingService()

        self.provider = settings.llm_provider.lower()

        self.model = settings.llm_model

        self.max_tokens = settings.llm_max_tokens

        # CRITICAL FIX: Gi·∫£m max_context_length ƒë·ªÉ tr√°nh MAX_TOKENS error
        self.max_context_length = min(12000, settings.rag_max_context_length)
        
        # CRITICAL FIX: TƒÉng max_output_tokens cho Gemini
        # CRITICAL FIX: TƒÉng max_output_tokens cho c√¢u tr·∫£ l·ªùi d√†i
        self.max_output_tokens = 12000  # Increased from 8192

        self._openai_client = None

        self._gemini_api_key = None

        self._gemini_base_url = "https://generativelanguage.googleapis.com/v1/models"

        

        # Initialize OpenAI client if needed

        if self.provider == "openai" and settings.openai_api_key:

            try:

                from openai import OpenAI

                self._openai_client = OpenAI(api_key=settings.openai_api_key)

                print(f"[RAG] OpenAI client initialized successfully with model: {self.model}")

            except Exception as e:

                print(f"[RAG] Failed to initialize OpenAI client: {e}")

                import traceback

                print(f"[RAG] Traceback: {traceback.format_exc()}")

                self._openai_client = None

        

        # Initialize Gemini if needed

        if self.provider == "gemini":

            self._gemini_api_key = settings.gemini_api_key

            if self._gemini_api_key:

                print(f"[RAG] Gemini API initialized successfully with model: {self.model}")

            else:

                print("[RAG] Gemini API key not found, falling back to local generation")

                self.provider = "local"

        

        if self.provider == "openai" and self._openai_client is None:

            print("[RAG] Falling back to local generation mode")

            self.provider = "local"
    
    def _determine_max_chunks_for_query(self, question: str, query_type: str) -> int:
        """Dynamically determine max chunks based on query complexity."""
        q_lower = question.lower()
        
        # CRITICAL FIX: DOCUMENT_OVERVIEW c·∫ßn NHI·ªÄU chunks nh·∫•t
        if query_type == "DOCUMENT_OVERVIEW":
            return 50  # ƒê·ªß ƒë·ªÉ cover to√†n b·ªô m·ª•c l·ª•c + overview chunks
        
        # CRITICAL FIX: SECTION_OVERVIEW c·∫ßn chunks v·ª´a ph·∫£i
        if query_type == "SECTION_OVERVIEW":
            return 30  # ƒê·ªß ƒë·ªÉ cover to√†n b·ªô subsections c·ªßa 1 ph·∫ßn
        
        # Tier 4 (Reasoning): C·∫ßn nhi·ªÅu chunks
        if query_type in ["MULTI_CONCEPT_REASONING", "CODE_ANALYSIS", "EXERCISE_GENERATION"]:
            # Count concepts mentioned
            concept_keywords = [
                "hoisting", "scope", "closure", "function", "arrow", "class",
                "object", "array", "loop", "for", "while", "if", "variable",
                "const", "let", "var", "promise", "async", "callback"
            ]
            concepts_found = sum(1 for kw in concept_keywords if kw in q_lower)
            
            if concepts_found >= 3:
                return 30
            elif concepts_found >= 2:
                return 25
            else:
                return 20
        
        # Tier 2 (Compare/Synthesize): C·∫ßn chunks t·ª´ nhi·ªÅu sections
        if query_type in ["COMPARE_SYNTHESIZE", "COMPARE"]:
            # Check if comparing 2+ items
            if any(word in q_lower for word in ["v√†", "v·ªõi", "so v·ªõi", ","]):
                return 25  # Need chunks for multiple items
            return 20
        
        # Tier 3 (List all / Enumerate)
        if any(kw in q_lower for kw in ["li·ªát k√™", "t·∫•t c·∫£", "bao nhi√™u", "cho v√≠ d·ª•"]):
            return 20  # Need more chunks to find all items
        
        # Tier 1 (Basic retrieval)
        return 15  # Default
    
    def _extract_section_from_content(self, content: str, file_type: str) -> Optional[str]:
        """Extract section/heading from content if it looks like a heading.
        
        For DOCX/MD/TXT: Look for patterns like "7.1.2. Section Name" or short lines
        that could be headings.
        """
        if not content or file_type not in ["docx", "doc", "md", "txt"]:
            return None
        
        content = content.strip()
        
        # If content is very short (likely a heading), check if it matches heading patterns
        if len(content) > 150:  # Too long to be a heading
            return None
        
        # Pattern 1: Numbered sections like "7.1.2. Section Name" or "1.2.3 Section Name"
        import re
        numbered_section_pattern = r'^(\d+\.)+\s*\d+[\.\s]+(.+)$'
        match = re.match(numbered_section_pattern, content)
        if match:
            # Return the full content as it's likely a section heading
            return content
        
        # Pattern 2: Short lines that might be headings (less than 80 chars, ends with colon or no period)
        if len(content) < 80 and not content.endswith('.'):
            # Could be a heading, but be more careful
            # Check if it has structure like "SECTION NAME:" or "Section Name"
            if ':' in content or content.isupper() or (len(content.split()) <= 10 and not content.endswith('.')):
                return content
        
        # Pattern 3: Markdown-style headings (already handled by parser, but just in case)
        if content.startswith('#'):
            return content.lstrip('#').strip()
        
        return None
    
    def _is_numbered_section(self, text: str) -> bool:
        """Check if text looks like a numbered section heading (e.g., '7.2.2. Section Name')."""
        if not text:
            return False
        import re
        # Pattern for numbered sections: starts with digits and dots like "7.2.2." or "1.2.3 "
        numbered_pattern = r'^(\d+\.)+\s*\d+[\.\s]+'
        return bool(re.match(numbered_pattern, text.strip()))

    def _is_fallback_answer(self, answer: str) -> bool:
        """Enhanced fallback detection."""
        if not answer or len(answer.strip()) < 20:
            return True
        
        answer_lower = answer.lower()
        fallback_patterns = [
            "kh√¥ng ƒë·ªß th√¥ng tin",
            "kh√¥ng t√¨m th·∫•y",
            "kh√¥ng th·ªÉ tr·∫£ l·ªùi",
            "t√†i li·ªáu kh√¥ng ƒë·ªÅ c·∫≠p",
            "kh√¥ng c√≥ d·ªØ li·ªáu",
            "kh√¥ng c√≥ trong t√†i li·ªáu",
            "t√†i li·ªáu kh√¥ng cung c·∫•p",
            "kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p",
            "kh√¥ng n·∫±m trong n·ªôi dung",
            "kh√¥ng c√≥ th√¥ng tin v·ªÅ",
            "ch∆∞a c√≥ ƒë·ªß d·ªØ li·ªáu",
            "kh√¥ng n√≥i v·ªÅ",
            "kh√¥ng nh·∫Øc ƒë·∫øn",
            "document does not",
            "no information",
            "cannot answer",
        ]
        
        return any(pattern in answer_lower for pattern in fallback_patterns)
    
    def _safe_parse_json(self, raw: str, query_type: str = "DIRECT") -> dict:
        """Safe JSON parsing v·ªõi comprehensive fallback v√† text reconstruction."""
        cleaned = raw.strip()
        
        # CRITICAL FIX: If LLM returns plain text instead of JSON, try to extract
        # Check if response looks like plain text (doesn't start with {)
        if not cleaned.startswith('{'):
            print(f"[RAG] ‚ö†Ô∏è LLM returned plain text, not JSON. Attempting recovery...")
            print(f"[RAG] Raw text (first 200 chars): {cleaned[:200]}")
            
            # ENHANCED: Try multiple JSON extraction methods
            methods = [
                # Method 1: Find first { to last }
                lambda s: re.search(r'\{.*\}', s, re.DOTALL),
                # Method 2: Find ```json blocks
                lambda s: re.search(r'```json\s*(\{.*?\})\s*```', s, re.DOTALL),
                # Method 3: Find after "answer":" pattern
                lambda s: re.search(r'"answer"\s*:\s*".*?".*?\}', s, re.DOTALL),
            ]
            
            for method in methods:
                match = method(cleaned)
                if match:
                    try:
                        json_str = match.group(1) if match.lastindex else match.group(0)
                        parsed = json.loads(json_str)
                        print(f"[RAG] ‚úÖ Extracted JSON using method")
                        return parsed
                    except Exception as e:
                        print(f"[RAG] Method failed: {e}")
                        continue
            
            # Method 4: Reconstruct JSON from text
            print(f"[RAG] Attempting text-to-JSON reconstruction...")
            return self._reconstruct_json_from_text(cleaned, query_type)
        
        # Remove markdown blocks
        if cleaned.startswith("```"):
            cleaned = re.sub(r'^```json?\s*', '', cleaned)
            cleaned = re.sub(r'\s*```$', '', cleaned)
            cleaned = cleaned.strip()
        
        # Try direct parse
        try:
            parsed = json.loads(cleaned)
            # NEW: Validate SECTION_OVERVIEW responses
            if parsed.get("answer_type") == "SECTION_OVERVIEW":
                answer = parsed.get("answer", "")
                has_title = bool(re.search(r'PH·∫¶N\s+\d+:', answer))
                has_list_intro = "N·ªôi dung ch√≠nh bao g·ªìm" in answer or "bao g·ªìm:" in answer
                topic_count = len(re.findall(r'\d+\.\s+\*\*', answer))
                
                if not has_title or not has_list_intro or topic_count < 3:
                    print(f"[RAG] ‚ö†Ô∏è SECTION_OVERVIEW format invalid:")
                    print(f"  - Has title: {has_title}")
                    print(f"  - Has list intro: {has_list_intro}")
                    print(f"  - Topic count: {topic_count}")
                    # Don't fail, but log warning
            return parsed
        except json.JSONDecodeError as e:
            print(f"[RAG] JSON decode error: {e}")
            pass
        
        # Try to extract JSON object
        match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned, re.DOTALL)
        if match:
            try:
                return json.loads(match.group())
            except Exception as e:
                print(f"[RAG] Extracted JSON parse failed: {e}")
                pass
        
        # Final fallback: reconstruct from text
        print(f"[RAG] All parsing attempts failed. Attempting reconstruction...")
        return self._reconstruct_json_from_text(cleaned, query_type)
    
    def _reconstruct_json_from_text(self, text: str, query_type: str) -> dict:
        """Reconstruct JSON from plain text answer (fallback)."""
        
        # Extract chunks mentioned in text
        chunk_pattern = r'\[Chunk\s+(\d+)\]|chunk\s+(\d+)'
        chunks_found = []
        for match in re.finditer(chunk_pattern, text, re.IGNORECASE):
            chunk_num = match.group(1) or match.group(2)
            if chunk_num:
                chunks_found.append(int(chunk_num))
        chunks_found = list(set(chunks_found))  # Deduplicate
        
        # Determine answer type from content
        text_lower = text.lower()
        answer_type = "FALLBACK"
        confidence = 0.0
        
        if query_type == "SECTION_OVERVIEW" or any(marker in text_lower for marker in ['ph·∫ßn', 'n·ªôi dung ch√≠nh', 'bao g·ªìm']):
            answer_type = "SECTION_OVERVIEW"
            confidence = 0.75 if chunks_found else 0.5
        elif query_type == "CODE_ANALYSIS" and "ph√¢n t√≠ch" in text_lower:
            answer_type = "CODE_ANALYSIS"
            confidence = 0.7
        elif query_type == "MULTI_CONCEPT_REASONING":
            answer_type = "MULTI_CONCEPT_REASONING"
            confidence = 0.65
        elif query_type == "EXERCISE_GENERATION" and ("b√†i t·∫≠p" in text_lower or "function" in text_lower):
            answer_type = "EXERCISE_GENERATION"
            confidence = 0.7
        elif chunks_found:
            answer_type = "DIRECT"
            confidence = 0.6
        
        # Extract main answer (first 1000 chars or until double newline)
        answer_match = re.search(r'^(.+?)(?:\n\n|$)', text, re.DOTALL)
        answer = answer_match.group(1) if answer_match else text[:1000]
        answer = answer.strip()
        
        print(f"[RAG] Reconstructed JSON: answer_type={answer_type}, confidence={confidence:.2f}, chunks={len(chunks_found)}")
        
        return {
            "answer": answer,
            "answer_type": answer_type,
            "chunks_used": chunks_found[:10] if chunks_found else [],
            "confidence": confidence,
            "sentence_mapping": [],
            "sources": {
                "from_document": bool(chunks_found),
                "from_external_knowledge": False
            },
            "_reconstructed": True  # Flag for debugging
        }
    
    def _get_fallback_response(self) -> tuple:
        """Safe fallback response."""
        return (
            "Hi·ªán t·∫°i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y. Vui l√≤ng th·ª≠ l·∫°i.",
            [],
            "FALLBACK",
            0.0,
            []
        )



    async def ask(

        self,

        db: AsyncIOMotorDatabase,

        user_id: str,

        question: str,

        document_id: Optional[str] = None,

        top_k: Optional[int] = None,

        conversation_id: Optional[str] = None,

    ) -> dict:

        documents: List[DocumentInDB] = []

        if document_id:

            doc = await get_document_by_id(db, document_id)

            if not doc or doc.user_id != user_id:

                raise ValueError("Document not found or not accessible")

            documents = [doc]
            
        else:

            documents = await get_documents_by_user(db, user_id)

        # Handle c√°c c√¢u ch√†o / small-talk kh√¥ng li√™n quan ƒë·∫øn t√†i li·ªáu
        normalized_question = question.strip().lower()
        small_talk_phrases = [
            "hi",
            "hello",
            "xin ch√†o",
            "ch√†o",
            "chao",
            "ch√†o b·∫°n",
            "ch√†o ad",
            "ch√†o admin",
            "good morning",
            "good afternoon",
            "good evening",
            "bye",
            "t·∫°m bi·ªát",
            "c·∫£m ∆°n",
            "thank you",
        ]

        # N·∫øu c√¢u h·ªèi r·∫•t ng·∫Øn v√† ch·ªâ l√† l·ªùi ch√†o / c·∫£m ∆°n th√¨ kh√¥ng g·ªçi RAG
        if len(normalized_question) <= 40 and any(
            normalized_question == p or normalized_question.startswith(p + " ")
            for p in small_talk_phrases
        ):
            # Tr·∫£ l·ªùi th√¢n thi·ªán, kh√¥ng tr√≠ch d·∫´n t√†i li·ªáu
            if any(
                kw in normalized_question
                for kw in ["c·∫£m ∆°n", "cam on", "thank", "tks", "thanks"]
            ):
                answer = (
                    "C·∫£m ∆°n b·∫°n! N·∫øu c·∫ßn m√¨nh h·ªó tr·ª£ gi·∫£i b√†i ho·∫∑c t√≥m t·∫Øt n·ªôi dung trong t√†i li·ªáu, "
                    "h√£y g·ª≠i c√¢u h·ªèi nh√©."
                )
            elif any(
                kw in normalized_question
                for kw in ["bye", "t·∫°m bi·ªát", "tam biet", "good night"]
            ):
                answer = (
                    "T·∫°m bi·ªát b·∫°n, h·∫πn g·∫∑p l·∫°i! Khi n√†o c·∫ßn h·ªèi b√†i ho·∫∑c tra c·ª©u t√†i li·ªáu, c·ª© quay l·∫°i nh√©."
                )
            else:
                answer = (
                    "Xin ch√†o! M√¨nh l√† tr·ª£ l√Ω StudyQnA, m√¨nh s·∫Ω gi√∫p b·∫°n tr·∫£ l·ªùi c√°c c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu "
                    "b·∫°n ƒë√£ t·∫£i l√™n. B·∫°n c·ª© g·ª≠i c√¢u h·ªèi v·ªÅ n·ªôi dung c·∫ßn h·ªçc nh√©."
                )

            # L∆∞u l·ªãch s·ª≠ nh∆∞ng kh√¥ng c√≥ references
            history_record = await create_history(
                db, user_id, question, answer, [], document_id, conversation_id
            )

            final_conversation_id = conversation_id

            # Gi·ªØ logic conversation_id t∆∞∆°ng t·ª± nh√°nh b√¨nh th∆∞·ªùng
            if not final_conversation_id:
                final_conversation_id = history_record.id
                try:
                    from bson import ObjectId

                    await db["histories"].update_one(
                        {"_id": ObjectId(history_record.id)},
                        {"$set": {"conversation_id": history_record.id}},
                    )
                    history_record.conversation_id = history_record.id
                except Exception as e:
                    print(
                        f"[RAG] Warning: Failed to update conversation_id for small-talk history {history_record.id}: {e}"
                    )

            return {
                "answer": answer,
                "references": [],
                "documents": [],
                "conversation_id": final_conversation_id,
                "history_id": history_record.id,
            }

        question_embeddings = await self.embedding_service.embed_texts([question])

        if not question_embeddings:
            # ENHANCED: Detect query type even for error cases
            query_type = detect_query_type_fast(question)
            return {
                "answer": "Kh√¥ng th·ªÉ t·∫°o embedding cho c√¢u h·ªèi.",
                "references": [],
                "documents": [],
                "metadata": {
                    "answer_type": "FALLBACK",
                    "confidence": 0.0,
                    "query_type": query_type,
                    "chunks_selected": 0,
                    "chunks_used": 0,
                }
            }

        # ENHANCED: Detect query type FIRST ƒë·ªÉ ƒëi·ªÅu ch·ªânh search
        query_type = detect_query_type_fast(question)
        print(f"[RAG] Detected query type: {query_type}")

        query_vector = np.array(question_embeddings, dtype="float32")

        results = []

        # ENHANCED: Dynamic max_chunks
        max_chunks_for_query = self._determine_max_chunks_for_query(question, query_type)
        print(f"[RAG] Max chunks for this query: {max_chunks_for_query}")

        # Search with larger initial top_k for more candidates
        for doc in documents:

            namespace = doc.faiss_namespace or f"user_{doc.user_id}_doc_{doc.id}"

            index = load_faiss_index(namespace)

            if index is None or index.ntotal == 0:

                continue

            if index.d != query_vector.shape[1]:

                continue

            try:

                # ENHANCED: TƒÉng search_k cho reasoning queries
                if query_type in ["MULTI_CONCEPT_REASONING", "COMPARE_SYNTHESIZE", "CODE_ANALYSIS"]:
                    search_k = min(50, index.ntotal)  # Increased from 30
                else:
                    search_k = min(30, index.ntotal)

                distances, ids = index.search(query_vector, search_k)

            except Exception:

                continue

            for dist, vector_id in zip(distances[0], ids[0]):

                if vector_id == -1:

                    continue

                similarity = float(1.0 / (1.0 + dist))

                results.append(

                    {

                        "document": doc,

                        "namespace": namespace,

                        "vector_id": int(vector_id),

                        "similarity": similarity,

                    }

                )



        if not results:
            # ENHANCED: Detect query type even for error cases
            query_type = detect_query_type_fast(question)
            
            # N·∫øu ng∆∞·ªùi d√πng ƒë√£ ch·ªçn 1 t√†i li·ªáu c·ª• th·ªÉ m√† kh√¥ng t√¨m ƒë∆∞·ª£c ƒëo·∫°n vƒÉn n√†o
            # th√¨ tr·∫£ l·ªùi r√µ r√†ng l√† c√¢u h·ªèi kh√¥ng n·∫±m trong n·ªôi dung t√†i li·ªáu ƒë√≥
            if document_id:
                answer = (
                    "C√¢u h·ªèi n√†y kh√¥ng n·∫±m trong n·ªôi dung c·ªßa t√†i li·ªáu b·∫°n ƒë√£ ch·ªçn. "
                    'B·∫°n c√≥ th·ªÉ th·ª≠ l·∫°i v·ªõi ch·∫ø ƒë·ªô "T·∫•t c·∫£ t√†i li·ªáu" ho·∫∑c ch·ªçn m·ªôt t√†i li·ªáu kh√°c ph√π h·ª£p h∆°n.'
                )
            else:
                answer = "Kh√¥ng t√¨m th·∫•y ƒëo·∫°n vƒÉn ph√π h·ª£p trong t√†i li·ªáu c·ªßa b·∫°n."

            return {
                "answer": answer,
                "references": [],
                "documents": [doc.id for doc in documents],
                "metadata": {
                    "answer_type": "FALLBACK",
                    "confidence": 0.0,
                    "query_type": query_type,
                    "chunks_selected": 0,
                    "chunks_used": 0,
                }
            }



        # Boost chunks that contain keywords from the question

        question_lower = question.lower()

        question_keywords = [q.strip() for q in question_lower.replace("?", "").replace("!", "").split() if len(q.strip()) > 2]

        

        for item in results:

            item["original_similarity"] = item["similarity"]



        print(f"[RAG] Found {len(results)} candidate chunks, boosting by question keywords: {question_keywords[:5]}")



        # Cache content for boosting and later use

        for item in results:

            doc = item["document"]

            record = await db["embeddings"].find_one(

                {"document_id": doc.id, "vector_index": item["vector_id"]}

            )

            if not record:

                continue

            

            item["_record"] = record

            chunk_id = record.get("chunk_id")

            chunk_doc = None

            if chunk_id:

                try:

                    from bson import ObjectId

                    chunk_doc = await db["chunks"].find_one({"_id": ObjectId(chunk_id)})

                except Exception:

                    chunk_doc = await db["chunks"].find_one({"_id": chunk_id})
            
            # Debug: ki·ªÉm tra chunk_doc v√† metadata cho m·ªôt v√†i chunks
            chunk_idx = record.get("chunk_index") if record else None
            if chunk_idx in [1, 3, 8, 10, 11] and doc.file_type in ["docx", "doc"]:
                if chunk_doc:
                    metadata = chunk_doc.get("metadata", {})
                    print(f"[RAG] Query chunk {chunk_idx}: chunk_id={chunk_id}, found={chunk_doc is not None}, metadata={metadata}")
                else:
                    print(f"[RAG] Query chunk {chunk_idx}: chunk_id={chunk_id}, chunk_doc NOT FOUND")

            content = (chunk_doc or {}).get("content") or record.get("content") or ""

            item["_content"] = content

            item["_chunk_doc"] = chunk_doc

            content_lower = content.lower()



            # CRITICAL FIX: Boost main sections (PH·∫¶N, CH∆Ø∆†NG) for section questions
            is_main_section = False
            section_boost = 0.0
            
            # Check if this is a main section (PH·∫¶N X, CH∆Ø∆†NG X)
            if chunk_doc:
                chunk_metadata = chunk_doc.get("metadata", {}) or {}
                section = chunk_metadata.get("section") or chunk_metadata.get("heading") or ""
                section_lower = section.lower() if section else ""
                
                # Main section patterns
                main_section_patterns = [
                    r'^ph·∫ßn\s+\d+',  # PH·∫¶N 5
                    r'^ch∆∞∆°ng\s+\d+',  # CH∆Ø∆†NG 3
                    r'^ph·∫ßn\s+[ivx]+',  # PH·∫¶N V
                ]
                
                for pattern in main_section_patterns:
                    if re.match(pattern, section_lower):
                        is_main_section = True
                        section_boost = 0.5  # Strong boost for main sections
                        print(f"[RAG] Main section detected: {section} (chunk {record.get('chunk_index')})")
                        break
            
            # Boost if content contains question keywords
            keyword_matches = sum(1 for kw in question_keywords if kw in content_lower)



            # Special boost for section numbers

            if any(kw in ["ph·∫ßn", "ch∆∞∆°ng", "part"] for kw in question_keywords):

                question_numbers = re.findall(r'\d+', question_lower)

                all_subsection_patterns = re.findall(r'\b\d+\.\d+\b', question_lower)



                for num in question_numbers:

                    if f"ph·∫ßn {num}" in content_lower or f"ch∆∞∆°ng {num}" in content_lower or f"part {num}" in content_lower:

                        keyword_matches += 3

                        section_pattern = rf"{num}\.\d+"

                        if re.search(section_pattern, content_lower):

                            keyword_matches += 5

                        break



                for subsec in all_subsection_patterns:

                    if subsec in content_lower:

                        keyword_matches += 8

                        print(f"[RAG] Found exact subsection match: {subsec} in chunk {record.get('chunk_index', '?')}")



            # Apply boosts (combine keyword boost + section boost)
            total_boost = 0.0
            boost_details = []
            
            if keyword_matches > 0:
                keyword_boost = min(0.3, keyword_matches * 0.08)
                total_boost += keyword_boost
                item["keyword_matches"] = keyword_matches
                boost_details.append(f"keywords({keyword_matches})")
            
            # CRITICAL FIX: Add section boost for main sections
            if is_main_section:
                total_boost += section_boost
                boost_details.append("main_section")
            
            # CRITICAL FIX: Boost chunks ch·ª©a M·ª§C L·ª§C ho·∫∑c headings ch√≠nh
            # Pattern 1: Boost chunks ch·ª©a "M·ª§C L·ª§C"
            if "m·ª•c l·ª•c" in content_lower or "table of contents" in content_lower:
                toc_boost = 0.8
                total_boost += toc_boost
                boost_details.append("table_of_contents")
                print(f"[RAG] Boosted chunk {record.get('chunk_index')} - contains TABLE OF CONTENTS")
            
            # Pattern 2: Boost chunks ch·ª©a nhi·ªÅu PH·∫¶N X
            # ƒê·∫øm s·ªë l∆∞·ª£ng "PH·∫¶N X" trong content
            section_count = len(re.findall(r'PH·∫¶N\s+\d+', content, re.IGNORECASE))
            if section_count >= 3:  # N·∫øu c√≥ t·ª´ 3 PH·∫¶N tr·ªü l√™n ‚Üí ƒë√¢y l√† chunk overview
                overview_boost = min(0.6, section_count * 0.15)
                total_boost += overview_boost
                boost_details.append(f"overview({section_count}_sections)")
                print(f"[RAG] Boosted chunk {record.get('chunk_index')} - contains {section_count} section headings")
            
            if total_boost > 0:
                item["similarity"] = min(1.0, item["similarity"] + total_boost)
                print(f"[RAG] Boosted chunk {record.get('chunk_index', '?')} by {total_boost:.3f} ({', '.join(boost_details)})")



        # Sort by boosted similarity

        sorted_results = sorted(results, key=lambda r: r["similarity"], reverse=True)



        # ENHANCED: Smart chunk selection based on query type
        selected_results = []
        priority_chunks = []
        regular_chunks = []

        # For reasoning queries, also prioritize chunks with related concepts
        if query_type in ["MULTI_CONCEPT_REASONING", "CODE_ANALYSIS", "COMPARE_SYNTHESIZE"]:
            concept_keywords = [
                "hoisting", "scope", "closure", "function", "arrow", "class",
                "object", "array", "loop", "for", "while", "if", "variable",
                "const", "let", "var", "promise", "async", "callback"
            ]
            
            for item in sorted_results:
                content_lower = (item.get("_content", "") or "").lower()
                has_concept = any(kw in content_lower for kw in concept_keywords)
                
                # Check section match
                has_section_match = False
                if any(kw in ["ph·∫ßn", "ch∆∞∆°ng", "part"] for kw in question_keywords):
                    question_numbers = re.findall(r'\d+', question_lower)
                    for num in question_numbers:
                        if (f"ph·∫ßn {num}" in content_lower or 
                            f"ch∆∞∆°ng {num}" in content_lower):
                            has_section_match = True
                            break
                
                if has_section_match or (has_concept and item["similarity"] > 0.4):
                    priority_chunks.append(item)
                else:
                    regular_chunks.append(item)
        else:
            # Original logic for other query types
            for item in sorted_results:
                content_lower = (item.get("_content", "") or "").lower()
                has_section_match = False
                
                if any(kw in ["ph·∫ßn", "ch∆∞∆°ng", "part"] for kw in question_keywords):
                    question_numbers = re.findall(r'\d+', question_lower)
                    for num in question_numbers:
                        if (f"ph·∫ßn {num}" in content_lower or 
                            f"ch∆∞∆°ng {num}" in content_lower or
                            re.search(rf"{num}\.\d+", content_lower)):
                            has_section_match = True
                            break
                
                if has_section_match:
                    priority_chunks.append(item)
                else:
                    regular_chunks.append(item)



        all_chunks_ordered = priority_chunks + regular_chunks



        current_context_length = 0

        chunk_metadata_for_context = []  # Store metadata to pass to LLM

        # ENHANCED: Use dynamic max_chunks based on query type
        max_selected_chunks = max_chunks_for_query

        for item in all_chunks_ordered:

            content = item.get("_content", "")

            content_length = len(content) if content else 0

            # CRITICAL FIX: Check c·∫£ context length V√Ä s·ªë chunks
            if (current_context_length + content_length + 500 > self.max_context_length) or \
               (len(selected_results) >= max_selected_chunks):
                break



            selected_results.append(item)

            

            # Store chunk metadata

            record = item.get("_record")

            chunk_doc = item.get("_chunk_doc")

            
            # L·∫•y metadata t·ª´ chunk_doc (chunks collection)
            chunk_metadata = {}
            if chunk_doc:
                chunk_metadata = (chunk_doc.get("metadata") or {})
                # Debug: log metadata cho m·ªôt v√†i chunks ƒë·ªÉ ki·ªÉm tra
                chunk_idx = record.get("chunk_index") if record else None
                if chunk_idx in [1, 3, 8, 10, 11] and item["document"].file_type in ["docx", "doc"]:
                    print(f"[RAG] Building context - chunk {chunk_idx}: chunk_doc metadata = {chunk_metadata}")
            else:
                # N·∫øu kh√¥ng c√≥ chunk_doc, th·ª≠ t·ª´ record
                if record:
                    chunk_metadata = (record.get("metadata") or {})
            
            # ƒê·∫£m b·∫£o chunk_metadata l√† dict
            if not isinstance(chunk_metadata, dict):
                chunk_metadata = {}

            # L·∫•y heading ho·∫∑c section title cho DOCX

            heading = chunk_metadata.get("heading") or chunk_metadata.get("title") or chunk_metadata.get("section_title")
            section = chunk_metadata.get("section")
            
            # N·∫øu kh√¥ng c√≥ section/heading trong metadata, th·ª≠ extract t·ª´ content
            if not section and not heading and content and item["document"].file_type in ["docx", "doc", "md", "txt"]:
                extracted = self._extract_section_from_content(content, item["document"].file_type)
                if extracted:
                    section = extracted
                    # N·∫øu content ng·∫Øn v√† c√≥ v·∫ª l√† heading, th√¨ ƒë√≥ l√† heading
                    if len(content.strip()) < 100:
                        heading = extracted

            chunk_idx = record.get("chunk_index") if record else None
            
            chunk_metadata_for_context.append({

                "chunk_index": chunk_idx,

                "document_id": record.get("document_id") if record else None,  # TH√äM document_id

                "page_number": chunk_metadata.get("page_number"),

                "section": section,

                "heading": heading,  # TH√äM heading

                "document_type": item["document"].file_type,  # TH√äM lo·∫°i file

                "document_filename": item["document"].filename,  # TH√äM t√™n file

                "content": content

            })
            
            # Debug: log section/heading cho m·ªôt v√†i chunks
            if chunk_idx in [1, 3, 8, 10, 11] and item["document"].file_type in ["docx", "doc"]:
                print(f"[RAG] Context metadata for chunk {chunk_idx}: section={section}, heading={heading}, extracted_from_content={section != chunk_metadata.get('section')}")

            

            current_context_length += content_length



        print(f"[RAG] Selected {len(selected_results)} chunks (context length: {current_context_length}/{self.max_context_length} chars)")

        print(f"[RAG] Priority chunks: {len(priority_chunks)}, Regular chunks: {len(regular_chunks)}")



        # Add similarity to chunk metadata
        for item in selected_results:
            record = item.get("_record")
            if record:
                chunk_idx = record.get("chunk_index")
                for chunk_meta in chunk_metadata_for_context:
                    if chunk_meta.get("chunk_index") == chunk_idx:
                        chunk_meta["similarity"] = item.get("similarity", 0.5)
                        break
        
        # ENHANCED: Generate answer with query_type passed to prompt builder
        answer, chunks_actually_used, answer_type, confidence, sentence_mapping = \
            await self._generate_answer_with_tracking(
                question, 
                chunk_metadata_for_context,
                query_type  # Pass query type to generation
            )

        print(f"[RAG] LLM used {len(chunks_actually_used)} chunks in answer")
        print(f"[RAG] Chunks used: {chunks_actually_used}")
        print(f"[RAG] Answer type: {answer_type}, Confidence: {confidence:.2f}")



        # Build references from chunks actually used in answer
        # If LLM didn't return chunk indices, use all selected chunks (sorted by similarity)
        
        # Create a metadata map from chunk_metadata_for_context for quick lookup
        # Key: (chunk_index, document_id) -> metadata dict
        metadata_map = {}
        for chunk_meta in chunk_metadata_for_context:
            key = (chunk_meta.get("chunk_index"), chunk_meta.get("document_id"))
            metadata_map[key] = chunk_meta
        
        # Second pass: Fill in missing sections by looking backward at previous chunks
        # This ensures all chunks have section information if available from earlier chunks
        # Sort chunks by chunk_index within each document to ensure proper order
        chunks_by_doc = {}
        for chunk_meta in chunk_metadata_for_context:
            doc_id = chunk_meta.get("document_id")
            if doc_id not in chunks_by_doc:
                chunks_by_doc[doc_id] = []
            chunks_by_doc[doc_id].append(chunk_meta)
        
        # For each document, sort chunks by chunk_index and fill in missing sections
        for doc_id, doc_chunks in chunks_by_doc.items():
            # Sort by chunk_index
            doc_chunks.sort(key=lambda x: x.get("chunk_index") or 0)
            
            # Now iterate through chunks in order and fill in missing sections
            for i, chunk_meta in enumerate(doc_chunks):
                chunk_idx = chunk_meta.get("chunk_index")
                current_section = chunk_meta.get("section")
                current_heading = chunk_meta.get("heading")
                
                # If this chunk doesn't have a section/heading, try to find one from previous chunks
                if not current_section and not current_heading and chunk_idx is not None and chunk_idx > 0:
                    # Look backward through previous chunks in the same document
                    # Collect all potential sections and choose the best one
                    candidate_sections = []
                    
                    # Look backward through chunks we've already processed
                    for j in range(i - 1, -1, -1):
                        prev_chunk = doc_chunks[j]
                        prev_section = prev_chunk.get("section")
                        prev_heading = prev_chunk.get("heading")
                        
                        # Collect all sections/headings we find
                        if prev_heading:
                            candidate_sections.append({
                                "text": prev_heading,
                                "is_heading": True,
                                "is_numbered": self._is_numbered_section(prev_heading),
                                "length": len(prev_heading),
                                "chunk_idx": prev_chunk.get("chunk_index")
                            })
                        if prev_section and prev_section != prev_heading:
                            candidate_sections.append({
                                "text": prev_section,
                                "is_heading": False,
                                "is_numbered": self._is_numbered_section(prev_section),
                                "length": len(prev_section),
                                "chunk_idx": prev_chunk.get("chunk_index")
                            })
                    
                    # Choose the best section: prefer numbered sections, then short headings
                    found_section = None
                    found_heading = None
                    
                    if candidate_sections:
                        # Sort by priority:
                        # 1. Numbered sections (like "7.2.2. ...")
                        # 2. Short headings (< 60 chars)
                        # 3. Other sections
                        def section_priority(candidate):
                            priority = 0
                            if candidate["is_numbered"]:
                                priority += 1000  # Highest priority
                            if candidate["is_heading"]:
                                priority += 100
                            if candidate["length"] < 60:
                                priority += 50
                            # Prefer sections from chunks closer to current chunk
                            priority += (100 - candidate["chunk_idx"] or 0) / 100
                            return priority
                        
                        candidate_sections.sort(key=section_priority, reverse=True)
                        best_candidate = candidate_sections[0]
                        found_section = best_candidate["text"]
                        found_heading = best_candidate["text"] if best_candidate["is_heading"] else None
                    
                    # If we found a section from a previous chunk, update this chunk's metadata
                    if found_section or found_heading:
                        chunk_meta["section"] = found_section
                        chunk_meta["heading"] = found_heading
                        # Also update the metadata_map entry
                        metadata_key = (chunk_idx, doc_id)
                        if metadata_key in metadata_map:
                            metadata_map[metadata_key]["section"] = found_section
                            metadata_map[metadata_key]["heading"] = found_heading
                            print(f"[RAG] Filled section for chunk {chunk_idx} from previous chunk: {found_section or found_heading}")
        
        # Count chunks per document BEFORE filtering (to determine which documents are most relevant)
        chunks_by_document = {}
        if chunks_actually_used:
            for chunk_info in chunks_actually_used:
                if isinstance(chunk_info, dict):
                    doc_id = chunk_info.get("document_id")
                else:
                    # Find document_id from selected_results
                    doc_id = None
                    for item in selected_results:
                        record = item.get("_record")
                        if record and record.get("chunk_index") == chunk_info:
                            doc_id = record.get("document_id")
                            break
                if doc_id:
                    chunks_by_document[doc_id] = chunks_by_document.get(doc_id, 0) + 1
        
        # === CRITICAL: Reference Logic ===
        final_references = []
        
        if answer_type in ["FALLBACK", "TOO_BROAD"]:
            # STRICT RULE: FALLBACK/TOO_BROAD = 0 references
            final_references = []
            print(f"[RAG] ‚úì {answer_type} detected ‚Üí 0 references enforced")
            
        elif not chunks_actually_used:
            # No chunks but not fallback ‚Üí suspicious
            if confidence > 0.5 and len(answer) > 100:
                # Try to infer from sentence mapping
                if sentence_mapping:
                    chunk_indices_from_mapping = [
                        s.get("chunk") for s in sentence_mapping 
                        if s.get("chunk") and not s.get("external", False)
                    ]
                    if chunk_indices_from_mapping:
                        print(f"[RAG] Recovered chunks from sentence_mapping: {chunk_indices_from_mapping}")
                        # Rebuild chunks_used
                        for idx in set(chunk_indices_from_mapping):
                            for item in selected_results:
                                record = item.get("_record")
                                if record and record.get("chunk_index") == idx:
                                    chunks_actually_used.append({
                                        "chunk_index": idx,
                                        "document_id": record.get("document_id")
                                    })
                                    break
                
                # If still no chunks, suspicious ‚Üí no refs
                if not chunks_actually_used:
                    print(f"[RAG] ‚ö† High confidence but no chunks ‚Üí suspicious, no refs")
                    final_references = []
            else:
                final_references = []
                print(f"[RAG] Low confidence + no chunks ‚Üí no references")
        
        if chunks_actually_used:
            # Build references from chunks_used
            final_references = self._build_references_from_chunks(
                chunks_actually_used, selected_results, chunk_metadata_for_context
            )
            print(f"[RAG] ‚úì Built {len(final_references)} references from chunks")
        
        # OLD LOGIC - REMOVED: If chunks_actually_used is empty, use top selected_results chunks
        # This is now handled above with strict fallback rules
        if False and not chunks_actually_used and selected_results:
            # Sort by similarity score (descending) and take top chunks
            # selected_results is already sorted (priority_chunks + regular_chunks), but ensure by similarity
            sorted_for_refs = sorted(selected_results, key=lambda r: r.get("similarity", 0), reverse=True)
            
            # Count chunks per document to determine relevance
            temp_chunks_by_doc = {}
            for item in sorted_for_refs[:15]:  # Check top 15
                record = item.get("_record")
                if record:
                    doc_id = record.get("document_id")
                    if doc_id:
                        temp_chunks_by_doc[doc_id] = temp_chunks_by_doc.get(doc_id, 0) + 1
            chunks_by_document = temp_chunks_by_doc
            
            # If document_id is specified, only use chunks from that document
            if document_id:
                top_chunks = [item for item in sorted_for_refs if item.get("_record", {}).get("document_id") == document_id][:15]
            else:
                # Filter: only keep chunks from documents with most chunks (if multiple documents)
                if len(chunks_by_document) > 1:
                    max_chunks = max(chunks_by_document.values())
                    # Only keep documents with at least 2 chunks, or if all have 1 chunk, keep all
                    if max_chunks >= 2:
                        relevant_docs = {doc_id for doc_id, count in chunks_by_document.items() if count >= 2}
                        if relevant_docs:
                            top_chunks = [item for item in sorted_for_refs 
                                        if item.get("_record", {}).get("document_id") in relevant_docs][:15]
                            print(f"[RAG] Filtered to documents with 2+ chunks: {relevant_docs}")
                        else:
                            top_chunks = sorted_for_refs[:15]
                    else:
                        top_chunks = sorted_for_refs[:15]
                else:
                    top_chunks = sorted_for_refs[:15]
            
            print(f"[RAG] No chunks returned by LLM, using top {len(top_chunks)} chunks (out of {len(selected_results)}) as references")
            # Convert selected_results to chunk_info format
            chunks_actually_used = []
            for item in top_chunks:
                record = item.get("_record")
                if record:
                    chunks_actually_used.append({
                        "chunk_index": record.get("chunk_index"),
                        "document_id": record.get("document_id")
                    })
            
            # Update chunks_by_document based on selected chunks
            chunks_by_document = {}
            for chunk_info in chunks_actually_used:
                doc_id = chunk_info.get("document_id")
                if doc_id:
                    chunks_by_document[doc_id] = chunks_by_document.get(doc_id, 0) + 1

        # OLD REFERENCE BUILDING CODE - Skip if we've already built references using new method
        # This old code manually builds references with detailed metadata extraction
        # We keep it as fallback but skip it when using the new _build_references_from_chunks method
        # Note: The new _build_references_from_chunks is called above, so this old code should rarely execute
        # Only execute if final_references is still empty (shouldn't happen with new logic)
        if not final_references and chunks_actually_used:
            # Old manual reference building code (kept for compatibility/fallback)
            # This should rarely execute since _build_references_from_chunks is called above
            for chunk_info in chunks_actually_used:
                # chunk_info c√≥ th·ªÉ l√† s·ªë (chunk_index) ho·∫∑c dict {"chunk_index": X, "document_id": Y}
                if isinstance(chunk_info, dict):
                    target_chunk_index = chunk_info.get("chunk_index")
                    target_document_id = chunk_info.get("document_id")
                else:
                    target_chunk_index = chunk_info
                    target_document_id = None

            

            # Find the corresponding item in selected_results

            for item in selected_results:

                record = item.get("_record")

                doc = item["document"]

                

                if not record:

                    continue

                

                # QUAN TR·ªåNG: Check c·∫£ chunk_index V√Ä document_id ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n

                chunk_index_match = record.get("chunk_index") == target_chunk_index

                

                # N·∫øu c√≥ document_id trong chunk_info, ph·∫£i match c·∫£ document_id

                if target_document_id:

                    document_id_match = record.get("document_id") == target_document_id

                    if not (chunk_index_match and document_id_match):

                        continue

                else:

                    # N·∫øu kh√¥ng c√≥ document_id, ch·ªâ c·∫ßn match chunk_index

                    if not chunk_index_match:

                        continue

                

                chunk_doc = item.get("_chunk_doc")

                content = item.get("_content")

                record = item.get("_record")

                

                # L·∫•y metadata - ∆∞u ti√™n t·ª´ metadata_map (ƒë√£ c√≥ s·∫µn t·ª´ chunk_metadata_for_context)
                # ƒê√¢y l√† ngu·ªìn ƒë√°ng tin c·∫≠y nh·∫•t v√¨ ƒë√£ ƒë∆∞·ª£c l·∫•y tr·ª±c ti·∫øp t·ª´ database khi build context
                metadata_key = (target_chunk_index, target_document_id or record.get("document_id") if record else None)
                cached_metadata = metadata_map.get(metadata_key)
                
                if cached_metadata:
                    # S·ª≠ d·ª•ng metadata t·ª´ cache (ƒë√£ c√≥ section, heading, page_number)
                    # Metadata map ƒë√£ ƒë∆∞·ª£c fill v·ªõi sections t·ª´ previous chunks trong second pass
                    page_number = cached_metadata.get("page_number")
                    section = cached_metadata.get("section")
                    heading = cached_metadata.get("heading")
                    display_section = section or heading
                    
                    # If the section is not a numbered section, we might find a better one from database
                    # So we'll still try to find a numbered section later if display_section is not numbered
                else:
                    # Fallback: l·∫•y t·ª´ database n·∫øu kh√¥ng c√≥ trong cache
                    chunk_metadata = {}
                    
                    # ∆Øu ti√™n metadata t·ª´ chunk_doc (chunks collection)
                    if chunk_doc:
                        chunk_metadata = (chunk_doc.get("metadata") or {})
                    
                    # N·∫øu kh√¥ng c√≥, th·ª≠ t·ª´ record (embeddings collection)
                    if not chunk_metadata and record:
                        chunk_metadata = (record.get("metadata") or {})
                    
                    # ƒê·∫£m b·∫£o chunk_metadata l√† dict
                    if not isinstance(chunk_metadata, dict):
                        chunk_metadata = {}
                    
                    page_number = chunk_metadata.get("page_number")
                    section = chunk_metadata.get("section")
                    heading = chunk_metadata.get("heading") or chunk_metadata.get("title") or chunk_metadata.get("section_title")
                    display_section = section or heading
                    
                    # Debug: log n·∫øu kh√¥ng t√¨m th·∫•y trong cache
                    if target_chunk_index in [1, 3, 8, 10, 11, 42, 31, 48, 66]:
                        print(f"[RAG] Warning: chunk {target_chunk_index} not found in metadata_map, using database. metadata={chunk_metadata}")
                
                # If still no section, try to extract from content (shouldn't happen often after second pass)
                if not display_section and content and doc.file_type:
                    extracted_section = self._extract_section_from_content(content, doc.file_type)
                    if extracted_section:
                        display_section = extracted_section
                        print(f"[RAG] Extracted section from content for chunk {target_chunk_index}: {display_section}")
                
                # If still no section, or if section is not numbered (might have better numbered section),
                # try to find from previous chunks in database
                # This handles cases where the chunk wasn't in chunk_metadata_for_context
                # or where we want to find a better numbered section
                has_section = bool(display_section)
                is_numbered = self._is_numbered_section(display_section) if display_section else False
                
                # Query database if: no section, or has section but not numbered (might find better numbered section)
                if target_chunk_index is not None and target_chunk_index > 0 and (not has_section or not is_numbered):
                    doc_id_for_lookup = target_document_id or record.get("document_id") if record else None
                    if doc_id_for_lookup:
                        try:
                            # Query database for previous chunks in the same document
                            # Look for chunks with chunk_index < target_chunk_index, ordered by chunk_index desc
                            # Limit to 10 chunks to avoid too many queries
                            previous_chunks = await db["chunks"].find({
                                "document_id": doc_id_for_lookup,
                                "chunk_index": {"$lt": target_chunk_index}
                            }).sort("chunk_index", -1).limit(10).to_list(length=10)
                            
                            # Collect candidate sections from previous chunks
                            candidate_sections = []
                            for prev_chunk in previous_chunks:
                                prev_metadata = prev_chunk.get("metadata") or {}
                                prev_section = prev_metadata.get("section")
                                prev_heading = prev_metadata.get("heading")
                                
                                # Also try to extract from content if it looks like a heading
                                prev_content = prev_chunk.get("content", "")
                                if not prev_section and not prev_heading and prev_content:
                                    extracted = self._extract_section_from_content(prev_content, doc.file_type if doc else "docx")
                                    if extracted:
                                        prev_section = extracted
                                
                                if prev_heading:
                                    candidate_sections.append({
                                    "text": prev_heading,
                                    "is_heading": True,
                                    "is_numbered": self._is_numbered_section(prev_heading),
                                    "length": len(prev_heading),
                                    "chunk_idx": prev_chunk.get("chunk_index")
                                })
                                if prev_section and prev_section != prev_heading:
                                    candidate_sections.append({
                                        "text": prev_section,
                                        "is_heading": False,
                                        "is_numbered": self._is_numbered_section(prev_section),
                                        "length": len(prev_section),
                                        "chunk_idx": prev_chunk.get("chunk_index")
                                    })
                            
                            # Choose the best section: prefer numbered sections
                            if candidate_sections:
                                def section_priority(candidate):
                                    priority = 0
                                    if candidate["is_numbered"]:
                                        priority += 1000  # Highest priority
                                    if candidate["is_heading"]:
                                        priority += 100
                                    if candidate["length"] < 60:
                                        priority += 50
                                    # Prefer sections from chunks closer to current chunk
                                    priority += (100 - candidate["chunk_idx"] or 0) / 100
                                    return priority
                                
                                candidate_sections.sort(key=section_priority, reverse=True)
                                best_candidate = candidate_sections[0]
                                
                                # Only use database section if:
                                # 1. We had no section, OR
                                # 2. Database has a numbered section (better than non-numbered)
                                if not has_section or (best_candidate["is_numbered"] and not is_numbered):
                                    display_section = best_candidate["text"]
                                    print(f"[RAG] Found section from database for chunk {target_chunk_index}: {display_section} (from chunk {best_candidate['chunk_idx']})")
                        except Exception as e:
                            print(f"[RAG] Error querying database for previous chunks: {e}")
                
                if not display_section:
                    print(f"[RAG] No section found for chunk {target_chunk_index} after all attempts")

                

                preview = content[:160] if content else None

                

                final_references.append(

                    HistoryReference(

                        document_id=doc.id,

                        document_filename=doc.filename,

                        document_file_type=doc.file_type,

                        chunk_id=str(chunk_doc.get("_id")) if chunk_doc else None,

                        chunk_index=target_chunk_index,

                        page_number=page_number,

                        section=display_section,  # S·ª≠ d·ª•ng section ho·∫∑c heading

                        score=item["similarity"],

                        content_preview=preview,

                    )

                )

                break



        # Remove duplicates - kh√°c bi·ªát gi·ªØa PDF v√† DOCX

        seen_keys = set()

        deduplicated_refs = []

        

        for ref in final_references:

            # V·ªõi PDF: deduplicate theo page

            if ref.document_file_type and ref.document_file_type.lower() == "pdf" and ref.page_number:

                page_key = f"{ref.document_id}_page_{ref.page_number}"

                if page_key in seen_keys:

                    continue

                seen_keys.add(page_key)

            

            # V·ªõi DOCX: deduplicate theo section ho·∫∑c chunk

            elif ref.document_file_type and ref.document_file_type.lower() in ["docx", "doc"]:

                if ref.section:

                    section_key = f"{ref.document_id}_section_{ref.section}"

                    if section_key in seen_keys:

                        continue

                    seen_keys.add(section_key)

                else:

                    # N·∫øu kh√¥ng c√≥ section, d√πng chunk_index

                    chunk_key = f"{ref.document_id}_chunk_{ref.chunk_index}"

                    if chunk_key in seen_keys:

                        continue

                    seen_keys.add(chunk_key)

            else:

                # Fallback: deduplicate theo chunk_index

                chunk_key = f"{ref.document_id}_chunk_{ref.chunk_index}"

                if chunk_key in seen_keys:

                    continue

                seen_keys.add(chunk_key)

            

            deduplicated_refs.append(ref)



        # Filter references based on document_id
        filtered_refs = deduplicated_refs
        
        if document_id:
            # Only keep references from the specified document
            filtered_refs = [ref for ref in deduplicated_refs if ref.document_id == document_id]
            print(f"[RAG] Filtered references to document {document_id}: {len(filtered_refs)} references")
        else:
            # ·ªû ch·∫ø ƒë·ªô "T·∫•t c·∫£ t√†i li·ªáu" th√¨ gi·ªØ m·ªçi t√†i li·ªáu m√† LLM ƒë√£ s·ª≠ d·ª•ng,
            # kh√¥ng l·ªçc b·ªõt theo chunks_by_document ƒë·ªÉ c√≥ th·ªÉ tr√≠ch d·∫´n t·ª´ nhi·ªÅu file kh√°c nhau.
            if len(chunks_by_document) > 1:
                print(f"[RAG] Multiple documents used in answer, keeping references from all {len(chunks_by_document)} documents")
            elif len(chunks_by_document) == 1:
                print(f"[RAG] Single document used in answer, keeping all references")
        
        # Smart filtering: If there are multiple sections, prioritize the section(s) v·ªõi nhi·ªÅu chunk nh·∫•t.
        # Ch·ªâ √°p d·ª•ng khi t·∫•t c·∫£ references ƒë·ªÅu thu·ªôc 1 t√†i li·ªáu; n·∫øu nhi·ªÅu t√†i li·ªáu th√¨ gi·ªØ nguy√™n.
        # CRITICAL FIX: Skip filtering for DOCUMENT_OVERVIEW
        if (
            len(filtered_refs) > 2
            and chunks_actually_used
            and answer_type != "DOCUMENT_OVERVIEW"  # ‚Üê ADD THIS CHECK
            and len({ref.document_id for ref in filtered_refs if ref.document_id}) == 1
        ):
            # Count chunks per section from chunks_actually_used
            section_chunk_counts = {}
            for chunk_info in chunks_actually_used:
                if isinstance(chunk_info, dict):
                    chunk_idx = chunk_info.get("chunk_index")
                    doc_id = chunk_info.get("document_id")
                else:
                    chunk_idx = chunk_info
                    doc_id = None
                
                # Find section from metadata_map
                metadata_key = (chunk_idx, doc_id)
                chunk_meta = metadata_map.get(metadata_key)
                if chunk_meta:
                    # Determine section key
                    file_type = chunk_meta.get("document_type", "").lower()
                    if file_type in ["docx", "doc"]:
                        section = chunk_meta.get("section") or chunk_meta.get("heading")
                        if section:
                            section_key = f"{doc_id}_{section}"
                        else:
                            section_key = f"{doc_id}_no_section_{chunk_idx}"
                    elif file_type == "pdf":
                        page = chunk_meta.get("page_number")
                        if page:
                            section_key = f"{doc_id}_page_{page}"
                        else:
                            section_key = f"{doc_id}_no_page_{chunk_idx}"
                    else:
                        section_key = f"{doc_id}_other_{chunk_idx}"
                    
                    section_chunk_counts[section_key] = section_chunk_counts.get(section_key, 0) + 1
            
            # Group filtered_refs by section
            section_refs = {}
            for ref in filtered_refs:
                if ref.document_file_type and ref.document_file_type.lower() in ["docx", "doc"]:
                    section_key = f"{ref.document_id}_{ref.section}" if ref.section else f"{ref.document_id}_no_section_{ref.chunk_index}"
                elif ref.document_file_type and ref.document_file_type.lower() == "pdf":
                    section_key = f"{ref.document_id}_page_{ref.page_number}" if ref.page_number else f"{ref.document_id}_no_page_{ref.chunk_index}"
                else:
                    section_key = f"{ref.document_id}_other_{ref.chunk_index}"
                
                if section_key not in section_refs:
                    section_refs[section_key] = []
                section_refs[section_key].append(ref)
            
            # Sort sections by chunk count (from chunks_actually_used), not by reference count
            sorted_sections = sorted(
                section_refs.items(), 
                key=lambda x: section_chunk_counts.get(x[0], 0), 
                reverse=True
            )
            
            # Keep references from top sections based on chunk count
            if sorted_sections:
                top_section_key = sorted_sections[0][0]
                top_chunk_count = section_chunk_counts.get(top_section_key, 0)
                
                if top_chunk_count >= 3:
                    # Only keep top section if it has 3+ chunks
                    filtered_refs = section_refs[top_section_key]
                    print(f"[RAG] Filtered to top section only: {top_section_key} ({top_chunk_count} chunks, {len(filtered_refs)} references)")
                elif len(sorted_sections) > 1:
                    # Keep top 2 sections
                    second_section_key = sorted_sections[1][0]
                    second_chunk_count = section_chunk_counts.get(second_section_key, 0)
                    filtered_refs = section_refs[top_section_key] + section_refs[second_section_key]
                    print(f"[RAG] Filtered to top 2 sections: {top_section_key} ({top_chunk_count} chunks), {second_section_key} ({second_chunk_count} chunks)")
                else:
                    # Only one section, keep all
                    filtered_refs = sorted_sections[0][1]

        # CRITICAL: For DOCUMENT_OVERVIEW, keep more references
        if answer_type == "DOCUMENT_OVERVIEW":
            final_references = filtered_refs[:10]  # Keep up to 10 refs instead of 5
        else:
            final_references = filtered_refs[:5]  # Limit to 5 references



        print(f"[RAG] Final references: {len(final_references)} chunks")

        print(f"[RAG] Reference details:")

        for ref in final_references:

            print(f"  - File: {ref.document_filename}, Page: {ref.page_number}, Section: {ref.section}, Chunk: {ref.chunk_index}")



        # === ENHANCED LOGGING ===
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "question": question[:100],
            "document_id": document_id,
            "query_type": detect_query_type_fast(question),
            "answer_type": answer_type,
            "confidence": confidence,
            "chunks_retrieved": len(results) if 'results' in locals() else 0,
            "chunks_selected": len(selected_results) if 'selected_results' in locals() else 0,
            "chunks_used": [c.get("chunk_index") for c in chunks_actually_used],
            "references_count": len(final_references),
            "answer_length": len(answer),
            "sentence_mapping_count": len(sentence_mapping),
            "max_similarity": max(
                [item.get("similarity", 0) for item in selected_results]
            ) if selected_results else 0
        }
        print(f"[RAG] Query Log: {json.dumps(log_entry, ensure_ascii=False)}")
        
        # Ensure conversation_id is set - use provided conversation_id or create new one
        # If conversation_id is provided, use it (for continuing existing conversation)
        # If not provided, we'll set it to history_id after creating the record
        final_conversation_id = conversation_id

        # Create history with conversation_id (may be None for new conversation)
        print(f"[RAG] ===== Creating history record =====")
        print(f"[RAG] Question: {question[:100]}...")
        print(f"[RAG] Answer: {answer[:100]}...")
        print(f"[RAG] Conversation ID: {final_conversation_id}")
        print(f"[RAG] Document ID: {document_id}")
        print(f"[RAG] References count: {len(final_references)}")
        
        history_record = await create_history(
            db, user_id, question, answer, final_references, document_id, final_conversation_id
        )
        
        print(f"[RAG] ‚úÖ History record created - ID: {history_record.id}")
        print(f"[RAG] ===================================")

        # If no conversation_id was provided, use the history_id as conversation_id
        # This ensures all Q&As in the same conversation share the same conversation_id
        if not final_conversation_id:
            final_conversation_id = history_record.id
            # Update the history record to set conversation_id = history_id
            # This ensures consistency when loading conversations
            try:
                from bson import ObjectId
                update_result = await db["histories"].update_one(
                    {"_id": ObjectId(history_record.id)},
                    {"$set": {"conversation_id": history_record.id}}
                )
                if update_result.modified_count > 0:
                    print(f"[RAG] Set conversation_id = history_id for new conversation: {history_record.id}")
                # Also update the history_record object for return value
                history_record.conversation_id = history_record.id
            except Exception as e:
                print(f"[RAG] Warning: Failed to update conversation_id for history {history_record.id}: {e}")
        else:
            # Conversation_id was provided, ensure it's set correctly in the record
            # (It should already be set, but double-check for consistency)
            try:
                from bson import ObjectId
                # Verify the record has the correct conversation_id
                existing_record = await db["histories"].find_one({"_id": ObjectId(history_record.id)})
                if existing_record and existing_record.get("conversation_id") != final_conversation_id:
                    await db["histories"].update_one(
                        {"_id": ObjectId(history_record.id)},
                        {"$set": {"conversation_id": final_conversation_id}}
                    )
                    print(f"[RAG] Updated conversation_id to {final_conversation_id} for history {history_record.id}")
            except Exception as e:
                print(f"[RAG] Warning: Failed to verify conversation_id for history {history_record.id}: {e}")



        return {
            "answer": answer,
            "references": final_references,
            "documents": list(set([ref.document_id for ref in final_references if ref.document_id])),
            "conversation_id": final_conversation_id,
            "history_id": history_record.id,
            "metadata": {
                "answer_type": answer_type,
                "confidence": confidence,
                "query_type": query_type,
                "chunks_selected": len(selected_results),
                "chunks_used": len(chunks_actually_used),
            }
        }



    async def _generate_answer_with_tracking(
        self, 
        question: str, 
        chunk_metadata_list: List[dict],
        query_type: str = "DIRECT"
    ) -> tuple[str, List[dict], str, float, List[dict]]:
        """
        Generate answer and track which chunks were actually used.
        Returns: (answer, chunks_used, answer_type, confidence, sentence_mapping)
        """
        # Build context v·ªõi similarity scores
        context_parts = []
        chunk_similarities = []

        for chunk_meta in chunk_metadata_list:
            idx = chunk_meta["chunk_index"]
            content = chunk_meta["content"]
            sim = chunk_meta.get("similarity", 0.5)
            chunk_similarities.append(sim)
            
            # Compact marker format
            parts = [f"[Chunk {idx}]"]
            if chunk_meta.get("document_filename"):
                parts.append(f"[{chunk_meta['document_filename']}]")
            if chunk_meta.get("page_number"):
                parts.append(f"[Page {chunk_meta['page_number']}]")
            if chunk_meta.get("section"):
                parts.append(f"[{chunk_meta['section']}]")
            parts.append(f"[Sim:{sim:.2f}]")
            
            marker = " ".join(parts)
            context_parts.append(f"{marker}\n{content}")
        
        context_text = "\n\n---\n\n".join(context_parts)
        
        # ENHANCED: Build prompt with query_type
        prompt = build_gemini_optimized_prompt(
            question=question,
            context_text=context_text,
            chunk_similarities=chunk_similarities,
            query_type=query_type
        )
        
        # Call Gemini API
        # Note: Gemini API uses camelCase, not snake_case
        # Note: responseMimeType is not supported by Gemini 2.5 Flash, so we parse JSON from text
        # CRITICAL FIX: TƒÉng maxOutputTokens l√™n 8192 ƒë·ªÉ tr√°nh MAX_TOKENS error
        generation_config = {
            "temperature": 0.0,
            "maxOutputTokens": self.max_output_tokens,  # 12000 tokens for long answers
            "candidateCount": 1,
            "stopSequences": [],
        }
        
        try:
            url = f"{self._gemini_base_url}/{self.model}:generateContent"
            payload = {
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": generation_config
            }
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    url,
                    params={"key": self._gemini_api_key},
                    json=payload
                )
                
                # Log error details if request fails
                if response.status_code != 200:
                    error_detail = response.text
                    print(f"[RAG] Gemini API error ({response.status_code}): {error_detail[:500]}")
                    try:
                        error_json = response.json()
                        print(f"[RAG] Error JSON: {json.dumps(error_json, ensure_ascii=False, indent=2)}")
                    except:
                        pass
                    raise Exception(f"Gemini API returned {response.status_code}")
                
                response.raise_for_status()
                data = response.json()
                
                # Safely extract text from response (similar to quiz_generator)
                raw = None
                
                if "candidates" not in data or len(data["candidates"]) == 0:
                    print(f"[RAG] No candidates in response. Full response: {json.dumps(data, indent=2, ensure_ascii=False)[:1000]}")
                    raise Exception("No candidates in Gemini response")
                
                candidate = data["candidates"][0]
                
                # Try multiple ways to extract text (like quiz_generator)
                # Case 1: Standard structure: candidates[0].content.parts[0].text
                if "content" in candidate:
                    content = candidate["content"]
                    if isinstance(content, dict) and "parts" in content:
                        parts = content["parts"]
                        if isinstance(parts, list) and len(parts) > 0:
                            if isinstance(parts[0], dict) and "text" in parts[0]:
                                raw = parts[0]["text"]
                            elif isinstance(parts[0], str):
                                raw = parts[0]
                    
                    # Case 2: content might be a string directly
                    elif isinstance(content, str):
                        raw = content
                    
                    # Case 3: content might be a list
                    elif isinstance(content, list) and len(content) > 0:
                        first_item = content[0]
                        if isinstance(first_item, dict) and "text" in first_item:
                            raw = first_item["text"]
                        elif isinstance(first_item, str):
                            raw = first_item
                
                # Case 4: Check candidate directly
                if not raw:
                    if "text" in candidate:
                        raw = candidate["text"]
                    elif isinstance(candidate, str):
                        raw = candidate
                
                # Case 5: Recursive search (fallback)
                if not raw:
                    def extract_text_recursive(obj, depth=0):
                        if depth > 5:  # Prevent infinite recursion
                            return None
                        if isinstance(obj, str) and len(obj) > 10:
                            return obj
                        if isinstance(obj, dict):
                            if "text" in obj:
                                return obj["text"]
                            for value in obj.values():
                                result = extract_text_recursive(value, depth + 1)
                                if result:
                                    return result
                        elif isinstance(obj, list):
                            for item in obj:
                                result = extract_text_recursive(item, depth + 1)
                                if result:
                                    return result
                        return None
                    
                    raw = extract_text_recursive(candidate)
                
                if not raw:
                    print(f"[RAG] ‚ùå Could not extract text from candidate")
                    print(f"[RAG] Full candidate structure: {json.dumps(candidate, indent=2, ensure_ascii=False)[:2000]}")
                    raise Exception("Could not extract text from Gemini response")
                
                print(f"[RAG] ‚úÖ Extracted response: {len(raw)} chars")
                parsed = self._safe_parse_json(raw, query_type)
                
                answer = parsed.get("answer", "")
                answer_type = parsed.get("answer_type", "FALLBACK")
                chunk_indices = parsed.get("chunks_used", [])
                confidence = parsed.get("confidence", 0.0)
                sentence_mapping = parsed.get("sentence_mapping", [])
                sources = parsed.get("sources", {})
                reasoning_steps = parsed.get("reasoning_steps", [])
                
                # === VALIDATION LAYER ===
                
                # Rule 0: TOO_BROAD detection
                if answer_type == "TOO_BROAD":
                    chunk_indices = []
                    sentence_mapping = []
                    confidence = 0.0
                    print(f"[RAG] TOO_BROAD detected ‚Üí enforcing 0 chunks")
                
                # ENHANCED: Validation for reasoning queries - More lenient
                if query_type in ["CODE_ANALYSIS", "EXERCISE_GENERATION", "MULTI_CONCEPT_REASONING"]:
                    # NEW: More lenient - only reject if VERY short or VERY low confidence
                    if len(answer) < 50:  # Only reject if VERY short
                        print(f"[RAG] Reasoning query but answer too short ({len(answer)} chars)")
                        answer_type = "FALLBACK"
                        chunk_indices = []
                        confidence = 0.0
                    elif confidence < 0.4:  # Lower threshold (from 0.5 to 0.4)
                        print(f"[RAG] Low confidence for reasoning query ({confidence:.2f})")
                        answer_type = "FALLBACK"
                        chunk_indices = []
                        confidence = 0.0
                    else:
                        # Accept even without reasoning_steps field if answer is substantial
                        if not reasoning_steps:
                            print(f"[RAG] Reasoning answer accepted despite missing reasoning_steps field (answer length: {len(answer)})")
                else:
                    # Original validation for non-reasoning queries
                    # CRITICAL FIX: Don't apply fallback detection for SECTION_OVERVIEW or DOCUMENT_OVERVIEW
                    if answer_type not in ["SECTION_OVERVIEW", "DOCUMENT_OVERVIEW"]:
                        # Rule 1: Fallback detection via keywords
                        if self._is_fallback_answer(answer):
                            answer_type = "FALLBACK"
                            chunk_indices = []
                            confidence = 0.0
                            sentence_mapping = []
                            print(f"[RAG] Fallback detected via keywords")
                        
                        # Rule 2: Low confidence ‚Üí force fallback (unless TOO_BROAD)
                        if confidence < settings.rag_low_confidence_threshold and answer_type != "TOO_BROAD":
                            answer_type = "FALLBACK"
                            chunk_indices = []
                            sentence_mapping = []
                            print(f"[RAG] Low confidence ({confidence:.2f}) ‚Üí forced fallback")
                    else:
                        # SECTION_OVERVIEW or DOCUMENT_OVERVIEW: Only fallback if EXPLICITLY no chunks
                        if not chunk_indices:
                            answer_type = "FALLBACK"
                            confidence = 0.0
                            sentence_mapping = []
                            print(f"[RAG] {answer_type} but no chunks ‚Üí fallback")
                        else:
                            print(f"[RAG] {answer_type} with {len(chunk_indices)} chunks ‚Üí keeping answer")
                
                # Rule 3: CRITICAL - Enforce fallback=0 refs (and TOO_BROAD)
                if answer_type in ["FALLBACK", "TOO_BROAD"]:
                    chunk_indices = []
                    sentence_mapping = []
                    confidence = 0.0
                    print(f"[RAG] {answer_type} type ‚Üí enforcing 0 chunks")
                
                # Rule 4: No chunks but claims document source ‚Üí suspicious (skip for overviews)
                if not chunk_indices and sources.get("from_document") and answer_type not in ["SECTION_OVERVIEW", "DOCUMENT_OVERVIEW"]:
                    answer_type = "FALLBACK"
                    confidence = 0.0
                    print(f"[RAG] Suspicious: no chunks but claims document source")
                
                # Rule 5: Check sentence_mapping consistency (skip for overviews)
                if sentence_mapping and answer_type not in ["SECTION_OVERVIEW", "DOCUMENT_OVERVIEW"]:
                    external_count = sum(1 for s in sentence_mapping if s.get("external", False))
                    total_count = len(sentence_mapping)
                    if total_count > 0 and external_count / total_count > 0.5:
                        answer_type = "FALLBACK"
                        chunk_indices = []
                        sentence_mapping = []
                        print(f"[RAG] >50% external sentences ‚Üí forced fallback")
                
                # Map to full chunk info
                chunks_used = []
                for idx in chunk_indices:
                    for meta in chunk_metadata_list:
                        if meta.get("chunk_index") == idx:
                            chunks_used.append({
                                "chunk_index": idx,
                                "document_id": meta.get("document_id")
                            })
                            break
                
                print(f"[RAG] Answer type: {answer_type}, Confidence: {confidence:.2f}")
                print(f"[RAG] Chunks: {chunk_indices}, Sentences mapped: {len(sentence_mapping)}")
                
                return answer, chunks_used, answer_type, confidence, sentence_mapping
                
        except Exception as e:
            print(f"[RAG] Error calling Gemini API: {e}")
            import traceback
            print(f"[RAG] Traceback: {traceback.format_exc()}")
            return self._get_fallback_response()

        # Fallback for OpenAI or other providers
        if self.provider == "openai" and self._openai_client:
            try:
                # For OpenAI, use similar approach but with different format
                messages = [
                    {
                        "role": "system",
                        "content": "You are a professional study assistant. Answer questions based on context. Always return JSON format with answer, answer_type, chunks_used, confidence, sentence_mapping, and sources fields.",
                    },
                    {
                        "role": "user",
                        "content": prompt,
                    },
                ]

                response = await asyncio.to_thread(
                    self._openai_client.chat.completions.create,
                    model=self.model,
                    messages=messages,
                    max_tokens=self.max_tokens,
                    temperature=0.0,
                    response_format={"type": "json_object"},
                )
                raw = response.choices[0].message.content
                parsed = self._safe_parse_json(raw)
                
                answer = parsed.get("answer", "")
                answer_type = parsed.get("answer_type", "FALLBACK")
                chunk_indices = parsed.get("chunks_used", [])
                confidence = parsed.get("confidence", 0.0)
                sentence_mapping = parsed.get("sentence_mapping", [])
                
                # Apply same validation rules
                if self._is_fallback_answer(answer):
                    answer_type = "FALLBACK"
                    chunk_indices = []
                    confidence = 0.0
                    sentence_mapping = []
                
                if confidence < settings.rag_low_confidence_threshold:
                    answer_type = "FALLBACK"
                    chunk_indices = []
                    sentence_mapping = []
                
                if answer_type == "FALLBACK":
                    chunk_indices = []
                    sentence_mapping = []
                    confidence = 0.0
                
                chunks_used = []
                for idx in chunk_indices:
                    for meta in chunk_metadata_list:
                        if meta.get("chunk_index") == idx:
                            chunks_used.append({
                                "chunk_index": idx,
                                "document_id": meta.get("document_id")
                            })
                            break
                
                return answer, chunks_used, answer_type, confidence, sentence_mapping
            except Exception as e:
                print(f"[RAG] OpenAI API call failed: {e}")
                return self._get_fallback_response()
        
        return self._get_fallback_response()



    def _parse_answer_and_chunks(self, full_response: str, chunk_metadata_list: List[dict]) -> tuple[str, List[dict]]:

        """

        Parse LLM response to extract answer and chunk indices used.

        Expected format: 

        Answer text here...

        [CHUNKS_USED: 1, 5, 7]

        

        Returns: (answer, list of chunk info dicts with chunk_index and document_id)

        """

        # Look for [CHUNKS_USED: ...] pattern

        match = re.search(r'\[CHUNKS_USED:\s*([\d,\s]+)\]', full_response, re.IGNORECASE)

        

        if match:

            # Extract chunk numbers

            chunk_str = match.group(1)

            chunk_indices = [int(x.strip()) for x in chunk_str.split(',') if x.strip().isdigit()]

            

            # Map chunk indices to chunk info with document_id

            chunks_used = []

            for chunk_idx in chunk_indices:

                # Find corresponding metadata

                for chunk_meta in chunk_metadata_list:

                    if chunk_meta.get("chunk_index") == chunk_idx:

                        chunks_used.append({

                            "chunk_index": chunk_idx,

                            "document_id": chunk_meta.get("document_id")

                        })

                        break

                else:

                    # N·∫øu kh√¥ng t√¨m th·∫•y metadata, v·∫´n th√™m chunk_index

                    chunks_used.append({"chunk_index": chunk_idx, "document_id": None})

            

            # Remove the [CHUNKS_USED: ...] part from answer

            answer = re.sub(r'\[CHUNKS_USED:.*?\]', '', full_response, flags=re.IGNORECASE).strip()

            

            return answer, chunks_used

        else:

            # Fallback: try to infer from content

            # Look for "Chunk X" mentions in the response

            chunks_mentioned = re.findall(r'[Cc]hunk\s+(\d+)', full_response)

            chunk_indices = list(set(int(x) for x in chunks_mentioned))

            

            # Map to chunk info

            chunks_used = []

            for chunk_idx in chunk_indices:

                for chunk_meta in chunk_metadata_list:

                    if chunk_meta.get("chunk_index") == chunk_idx:

                        chunks_used.append({

                            "chunk_index": chunk_idx,

                            "document_id": chunk_meta.get("document_id")

                        })

                        break

                else:

                    chunks_used.append({"chunk_index": chunk_idx, "document_id": None})

            

            return full_response.strip(), chunks_used





    def _build_references_from_chunks(
        self,
        chunks_used: List[dict],
        selected_results: List[dict],
        chunk_metadata_for_context: List[dict]
    ) -> List[HistoryReference]:
        """Build references from chunks actually used."""
        final_references = []
        
        for chunk_info in chunks_used:
            chunk_idx = chunk_info.get("chunk_index")
            doc_id = chunk_info.get("document_id")
            
            # Find in selected_results
            for item in selected_results:
                record = item.get("_record")
                if not record:
                    continue
                
                if record.get("chunk_index") == chunk_idx and \
                   record.get("document_id") == doc_id:
                    
                    doc = item["document"]
                    chunk_doc = item.get("_chunk_doc")
                    content = item.get("_content", "")
                    
                    # Get metadata
                    chunk_metadata = {}
                    if chunk_doc:
                        chunk_metadata = chunk_doc.get("metadata", {}) or {}
                    elif record:
                        chunk_metadata = record.get("metadata", {}) or {}
                    
                    page = chunk_metadata.get("page_number")
                    section = chunk_metadata.get("section")
                    heading = chunk_metadata.get("heading")
                    
                    preview = content[:160] if content else None
                    
                    final_references.append(
                        HistoryReference(
                            document_id=doc.id,
                            document_filename=doc.filename,
                            document_file_type=doc.file_type,
                            chunk_id=str(chunk_doc.get("_id")) if chunk_doc else None,
                            chunk_index=chunk_idx,
                            page_number=page,
                            section=section or heading,
                            score=item.get("similarity", 0.5),
                            content_preview=preview,
                        )
                    )
                    break
        
        # Deduplicate
        seen = set()
        deduplicated = []
        for ref in final_references:
            key = f"{ref.document_id}_{ref.chunk_index}"
            if key not in seen:
                seen.add(key)
                deduplicated.append(ref)
        
        # CRITICAL FIX: Adjust max references based on question complexity
        # Count how many chunks were actually used
        num_chunks_used = len(chunks_used)
        
        if num_chunks_used >= 5:  # Complex question - keep more refs
            max_refs = min(5, num_chunks_used)
        elif num_chunks_used >= 3:  # Medium complexity
            max_refs = min(4, num_chunks_used)
        else:  # Simple question
            max_refs = min(2, num_chunks_used)
        
        return deduplicated[:max_refs]


rag_service = RAGService()
